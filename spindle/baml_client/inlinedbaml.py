# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Using the new OpenAI Responses API for enhanced formatting\nclient<llm> CustomGPT5 {\n  provider openai-responses\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT5Mini {\n  provider openai-responses\n  retry_policy Exponential\n  options {\n    model \"gpt-5-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Openai with chat completion\nclient<llm> CustomGPT5Chat {\n  provider openai\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Latest Anthropic Claude 4 models\nclient<llm> CustomOpus4 {\n  provider anthropic\n  options {\n    model \"claude-opus-4-1-20250805\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet4 {\n  provider anthropic\n  options {\n    model \"claude-sonnet-4-20250514\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-5-haiku-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// Example Google AI client (uncomment to use)\n// client<llm> CustomGemini {\n//   provider google-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     api_key env.GOOGLE_API_KEY\n//   }\n// }\n\n// Example AWS Bedrock client (uncomment to use)\n// client<llm> CustomBedrock {\n//   provider aws-bedrock\n//   options {\n//     model \"anthropic.claude-sonnet-4-20250514-v1:0\"\n//     region \"us-east-1\"\n//     // AWS credentials are auto-detected from env vars\n//   }\n// }\n\n// Example Azure OpenAI client (uncomment to use)\n// client<llm> CustomAzure {\n//   provider azure-openai\n//   options {\n//     model \"gpt-5\"\n//     api_key env.AZURE_OPENAI_API_KEY\n//     base_url \"https://MY_RESOURCE_NAME.openai.azure.com/openai/deployments/MY_DEPLOYMENT_ID\"\n//     api_version \"2024-10-01-preview\"\n//   }\n// }\n\n// Example Vertex AI client (uncomment to use)\n// client<llm> CustomVertex {\n//   provider vertex-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     location \"us-central1\"\n//     // Uses Google Cloud Application Default Credentials\n//   }\n// }\n\n// Example Ollama client for local models (uncomment to use)\n// client<llm> CustomOllama {\n//   provider openai-generic\n//   options {\n//     base_url \"http://localhost:11434/v1\"\n//     model \"llama4\"\n//     default_role \"user\" // Most local models prefer the user role\n//     // No API key needed for local Ollama\n//   }\n// }\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT5Mini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT5Mini, CustomGPT5]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    // BAML creates a baml_client/ subdir inside this path, so we use \"..\"\n    // to generate directly into spindle/baml_client/\n    output_dir \"..\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.211.2\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "process.baml": "// Process DAG Extraction Schemas and Function\n\nenum ProcessStepType {\n  ACTIVITY @description(\"Linear action that advances the process\")\n  DECISION @description(\"Branching evaluation that selects one of multiple paths\")\n  EVENT @description(\"External trigger or milestone that influences the process\")\n  PARALLEL_GATEWAY @description(\"Point where multiple activities run concurrently\")\n  SUBPROCESS @description(\"Reference to a nested process or reusable routine\")\n}\n\nclass EvidenceSpan {\n  text string @description(\"Exact excerpt from the source that supports the step or dependency\")\n  start int? @description(\"Zero-based start index of the span within the text\")\n  end int? @description(\"End index (exclusive) of the span within the text\")\n}\n\nclass ProcessStep {\n  step_id string @description(\"Stable identifier used for references and dependencies\")\n  title string @description(\"Short human-readable name for the step\")\n  summary string @description(\"Concise explanation of what happens during this step\")\n  step_type ProcessStepType @description(\"Category that characterises the behaviour of the step\")\n  actors string[] @description(\"People, roles, or systems responsible for executing the step\")\n  inputs string[] @description(\"Key inputs, prerequisites, or artifacts consumed by this step\")\n  outputs string[] @description(\"Artifacts, decisions, or state transitions produced by this step\")\n  duration string? @description(\"Estimated timing or duration information if mentioned\")\n  prerequisites string[] @description(\"Step identifiers that must be completed before this step starts\")\n  evidence EvidenceSpan[] @description(\"Supporting text spans drawn from the source material\")\n}\n\nclass ProcessDependency {\n  from_step string @description(\"Identifier of the antecedent step\")\n  to_step string @description(\"Identifier of the dependent step\")\n  relation string @description(\"Nature of the dependency, e.g., 'precedes', 'blocks', 'enables'\")\n  condition string? @description(\"Conditional logic or guard that must be satisfied for the dependency\")\n  evidence EvidenceSpan[] @description(\"Supporting spans demonstrating this dependency in the text\")\n}\n\nclass ProcessGraph {\n  process_name string? @description(\"Human-readable name or title for the overall process\")\n  scope string? @description(\"Short description of the process boundaries or context\")\n  primary_goal string @description(\"Statement summarising the overall objective of the process\")\n  start_step_ids string[] @description(\"Identifiers of steps that have no incoming dependencies\")\n  end_step_ids string[] @description(\"Identifiers of steps that have no outgoing dependencies\")\n  steps ProcessStep[] @description(\"All steps that participate in the process\")\n  dependencies ProcessDependency[] @description(\"Directed edges connecting steps to form a DAG\")\n  notes string[] @description(\"Additional remarks, open questions, or caveats captured by the model\")\n}\n\nclass ProcessExtractionIssue {\n  code string @description(\"Machine-friendly identifier describing the issue\")\n  message string @description(\"Human-readable explanation of the issue or warning\")\n  related_step_ids string[] @description(\"Step identifiers relevant to this issue\")\n}\n\nclass ProcessExtractionResult {\n  status \"process_found\" | \"no_process\" | \"incomplete\" @description(\"Outcome of the extraction attempt\")\n  graph ProcessGraph? @description(\"Structured representation of the process when available\")\n  reasoning string @description(\"Narrative justification covering modelling choices and DAG structure\")\n  issues ProcessExtractionIssue[] @description(\"Non-fatal problems encountered during extraction\")\n}\n\nfunction ExtractProcessGraph(\n  text: string,\n  process_hint: string?,\n  existing_graph: ProcessGraph?\n) -> ProcessExtractionResult {\n  client CustomFast\n  prompt #\"\n    You are a process modelling specialist. Extract a directed acyclic graph (DAG) that captures the end-to-end process described in the provided text. Represent the process with well-defined steps and explicit dependencies.\n\n    {{ _.role(\"user\") }}\n\n    TEXT:\n    {{ text }}\n\n    {% if process_hint %}\n    PROCESS HINT:\n    {{ process_hint }}\n    {% endif %}\n\n    {% if existing_graph %}\n    EXISTING GRAPH CONTEXT:\n    - Process name: {{ existing_graph.process_name or \"Unknown\" }}\n    - Steps already known: {{ existing_graph.steps | map(attribute='step_id') | list }}\n    When possible, reuse step identifiers from the existing graph to maintain continuity.\n    {% endif %}\n\n    OUTPUT REQUIREMENTS:\n    1. Return status \"no_process\" if the text lacks procedural content.\n    2. Always ensure dependency targets reference defined step identifiers.\n    3. Identify start and end steps explicitly using computed arrays.\n    4. Capture conditional flows, loops described as repeated checks, and parallel work using step types and dependency relation labels.\n    5. Provide issues for ambiguous, conflicting, or incomplete information rather than guessing.\n    6. Summarise reasoning with numbered bullet points that justify the chosen structure and highlight any assumptions.\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n",
    "spindle.baml": "// Spindle: Knowledge Graph Triple Extraction\n// This file defines the data structures and extraction function for building knowledge graphs from text\n\n// Source metadata for tracking where a triple came from\nclass SourceMetadata {\n  source_name string @description(\"The name or identifier of the source document\")\n  source_url string? @description(\"Optional URL of the source document\")\n}\n\n// Character span indicating text evidence for a triple\nclass CharacterSpan {\n  text string @description(\"The exact text from the source that supports this triple\")\n  start int? @description(\"Starting character index in the source text (0-based, computed in post-processing)\")\n  end int? @description(\"Ending character index in the source text (exclusive, computed in post-processing)\")\n}\n\n// Attribute definition for entity types in the ontology\nclass AttributeDefinition {\n  name string @description(\"The name of the attribute (e.g., 'campaign_launch_dt', 'founded_date')\")\n  type string @description(\"Data type: 'string', 'int', 'float', 'bool', 'date', or a nested object schema definition\")\n  description string @description(\"A description of what this attribute represents\")\n}\n\n// Attribute value in extracted entities (includes type metadata)\nclass AttributeValue {\n  value string? @description(\"The extracted value as a string, or null if not found in the text\")\n  type string @description(\"The data type of this attribute (matches the type from AttributeDefinition)\")\n}\n\n// Structured entity in a triple\nclass Entity {\n  name string @description(\"The entity's name or identifier extracted from the text\")\n  type string @description(\"The entity type from the ontology (must match an EntityType.name exactly)\")\n  description string @description(\"A brief description of the entity generated from the text context\")\n  custom_atts map<string, AttributeValue> @description(\"Type-specific attributes as defined in the entity type's AttributeDefinition list\")\n}\n\n// A triple represents a subject-predicate-object relationship with supporting evidence\nclass Triple {\n  subject Entity @description(\"The subject entity with full metadata and attributes\")\n  predicate string @description(\"The relationship/predicate connecting subject and object\")\n  object Entity @description(\"The object entity with full metadata and attributes\")\n  source SourceMetadata @description(\"Metadata about the source of this triple\")\n  supporting_spans CharacterSpan[] @description(\"Character spans in the source text that support this triple\")\n  extraction_datetime string? @description(\"ISO 8601 datetime when this triple was extracted (set automatically in post-processing)\")\n}\n\n// Defines a type of entity that can appear in the knowledge graph\nclass EntityType {\n  name string @description(\"The name of the entity type (e.g., 'Person', 'Organization', 'Campaign')\")\n  description string @description(\"A description of what this entity type represents\")\n  attributes AttributeDefinition[] @description(\"List of custom attributes that should be extracted for entities of this type\")\n}\n\n// Defines a type of relationship between entities\nclass RelationType {\n  name string @description(\"The name of the relation type (e.g., 'works_at', 'located_in')\")\n  description string @description(\"A description of what this relation represents\")\n  domain string @description(\"The entity type that can be the subject of this relation\")\n  range string @description(\"The entity type that can be the object of this relation\")\n}\n\n// An ontology defines the structure of the knowledge graph\nclass Ontology {\n  entity_types EntityType[] @description(\"List of valid entity types\")\n  relation_types RelationType[] @description(\"List of valid relation types\")\n}\n\n// The result of triple extraction\nclass ExtractionResult {\n  triples Triple[] @description(\"List of extracted triples from the text\")\n  reasoning string @description(\"Explanation of the extraction decisions and entity consistency choices\")\n}\n\n// Result of ontology recommendation\nclass OntologyRecommendation {\n  ontology Ontology @description(\"The recommended ontology with entity and relation types\")\n  text_purpose string @description(\"Analysis of the text's overarching purpose or goal\")\n  reasoning string @description(\"Explanation of why these entity and relation types were recommended\")\n}\n\n// Result of ontology extension analysis\nclass OntologyExtension {\n  needs_extension bool @description(\"Whether the ontology needs to be extended for this text\")\n  new_entity_types EntityType[] @description(\"New entity types to add (empty if none needed)\")\n  new_relation_types RelationType[] @description(\"New relation types to add (empty if none needed)\")\n  critical_information_at_risk string @description(\"Description of what critical information would be lost without extension\")\n  reasoning string @description(\"Detailed explanation of why extension is or isn't needed\")\n}\n\n// Main function to extract knowledge graph triples from text\nfunction ExtractTriples(\n  text: string,\n  ontology: Ontology,\n  source_metadata: SourceMetadata,\n  existing_triples: Triple[]\n) -> ExtractionResult {\n  client CustomFast\n  prompt #\"\n    You are a knowledge graph extraction expert. Your task is to extract structured triples (subject-predicate-object) from the provided text, with rich entity metadata, custom attributes, and supporting evidence.\n\n    {{ _.role(\"user\") }}\n    \n    ONTOLOGY:\n    You must extract triples that conform to the following ontology:\n    \n    Valid Entity Types:\n    {% for entity_type in ontology.entity_types %}\n    - {{ entity_type.name }}: {{ entity_type.description }}\n      {% if entity_type.attributes|length > 0 %}\n      Custom Attributes:\n      {% for attr in entity_type.attributes %}\n        * {{ attr.name }} ({{ attr.type }}): {{ attr.description }}\n      {% endfor %}\n      {% endif %}\n    {% endfor %}\n    \n    Valid Relation Types:\n    {% for relation_type in ontology.relation_types %}\n    - {{ relation_type.name }}: {{ relation_type.description }}\n      (Domain: {{ relation_type.domain }}, Range: {{ relation_type.range }})\n    {% endfor %}\n    \n    SOURCE METADATA:\n    Source Name: {{ source_metadata.source_name }}\n    {% if source_metadata.source_url %}\n    Source URL: {{ source_metadata.source_url }}\n    {% endif %}\n    \n    EXISTING TRIPLES:\n    {% if existing_triples|length > 0 %}\n    The following triples have already been extracted from OTHER sources. You MUST:\n    1. Use consistent entity names - if an entity appears in existing triples, use the EXACT same name\n    2. Maintain entity identity - recognize when entities in the new text are the same as entities in existing triples\n    3. Duplicate triples ARE ALLOWED if they come from different sources (different source names)\n    4. If you extract the same fact that exists in existing triples, it's okay as long as it's from this new source\n    \n    {% for triple in existing_triples %}\n    - {{ triple.subject.name }} ({{ triple.subject.type }}) -> {{ triple.predicate }} -> {{ triple.object.name }} ({{ triple.object.type }}) [from: {{ triple.source.source_name }}]\n    {% endfor %}\n    {% else %}\n    This is the first extraction, so there are no existing triples to consider.\n    {% endif %}\n    \n    TEXT TO ANALYZE:\n    {{ text }}\n    \n    ENTITY EXTRACTION INSTRUCTIONS:\n    \n    For each entity in a triple (both subject and object), you MUST provide:\n    \n    1. **name**: The entity's name or identifier from the text\n       - Use clear, consistent names (e.g., \"John Smith\" not \"John\" or \"Smith\")\n       - If the entity appears in existing triples, use the EXACT same name\n    \n    2. **type**: The entity type from the ontology\n       - Must match exactly one of the EntityType names defined above\n       - Check domain/range constraints in relation types\n    \n    3. **description**: A brief description of the entity\n       - Generate this from the text context (1-2 sentences)\n       - Include relevant details that help identify or characterize the entity\n       - Be concise but informative\n    \n    4. **custom_atts**: Extract ALL attributes defined for this entity type\n       - Format: {\"attribute_name\": {\"value\": \"extracted_value\", \"type\": \"attribute_type\"}}\n       - Set \"value\" to null if the attribute is not mentioned in the text\n       - \"type\" must match the type from the AttributeDefinition\n       - Extract dates in ISO format (YYYY-MM-DD) when possible\n       - For nested objects, use JSON string representation\n    \n    TRIPLE EXTRACTION INSTRUCTIONS:\n    \n    1. Extract all meaningful triples from the text that conform to the ontology\n    2. Only use entity types and relation types defined in the ontology\n    3. For each triple, identify TEXT SPANS that provide evidence:\n       - Copy the EXACT text from the source that supports the triple\n       - You can include multiple text spans if evidence is spread across different parts\n       - Be precise - copy text exactly as it appears, including punctuation and spacing\n       - Include enough context to make the evidence clear\n    4. Provide reasoning explaining your extraction decisions\n    \n    EXAMPLE:\n    \n    Text: \"The Check Your A1C campaign launched on August 10, 2025, targeting diabetic members to remind them to get their A1C checked.\"\n    \n    Ontology has EntityType \"Campaign\" with attributes:\n    - campaign_launch_dt (date): Launch date\n    - campaign_completion_dt (date): Completion date\n    \n    Extracted entity for subject:\n    {\n      \"name\": \"Check Your A1C campaign\",\n      \"type\": \"Campaign\",\n      \"description\": \"A campaign targeting diabetic members to remind them to get their A1C checked\",\n      \"custom_atts\": {\n        \"campaign_launch_dt\": {\"value\": \"2025-08-10\", \"type\": \"date\"},\n        \"campaign_completion_dt\": {\"value\": null, \"type\": \"date\"}\n      }\n    }\n    \n    Note: Character indices for supporting spans will be computed automatically in post-processing.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Function to recommend an ontology based on text analysis\nfunction RecommendOntology(\n  text: string,\n  scope: string\n) -> OntologyRecommendation {\n  client CustomFast\n  prompt #\"\n    You are a knowledge graph ontology design expert. Your task is to analyze the provided text, understand its overarching purpose and domain, and recommend an appropriate ontology (entity types and relation types) that would be suitable for extracting knowledge from this and similar texts.\n\n    {{ _.role(\"user\") }}\n    \n    TEXT TO ANALYZE:\n    {{ text }}\n    \n    ONTOLOGY SCOPE: {{ scope }}\n    \n    GRANULARITY GUIDELINES:\n    \n    Your goal is to design an ontology with {{ scope }} scope. Use these principles to guide your decisions about how many types to create and how granular to make them.\n    \n    ENTITY TYPE GRANULARITY PRINCIPLES:\n    \n    1. **Abstraction Level**: Choose the right level of abstraction\n       - Too broad: \"Thing\", \"Entity\" (not useful)\n       - Too narrow: \"SoftwareEngineerAtGoogle\", \"CEOOfTechStartup\" (overfitting)\n       - Just right: \"Person\", \"Organization\", \"JobRole\" (reusable across instances)\n    \n    2. **Domain Relevance**: Include entity types that are central to THIS domain\n       - For medical text: Patient, Medication, Condition, Procedure, Hospital\n       - For business text: Company, Person, Investment, Product, Market\n       - Don't include types that rarely appear or aren't relevant to this specific text\n    \n    3. **Distinctiveness Test**: Only create separate types if they have:\n       - Different attributes or characteristics\n       - Different relationship patterns\n       - Different analytical value\n       - Example: \"Professor\" and \"Student\" are distinct if their relationships differ\n       - Example: \"CEO\" and \"CTO\" might both be \"ExecutiveRole\" if relationships are similar\n    \n    4. **Generalization**: Combine similar concepts under broader types when appropriate\n       - Instead of: \"Dog\", \"Cat\", \"Bird\" → use \"Animal\" (if distinctions don't matter)\n       - Instead of: \"Laptop\", \"Desktop\", \"Tablet\" → use \"Device\" or \"Computer\"\n       - BUT: Keep separate if the distinction is analytically important for this domain\n    \n    5. **Coverage**: Ensure types cover the main concepts in the text\n       - Don't create types for concepts that appear only once\n       - DO create types for recurring or structurally important concepts\n       - Focus on concepts that are central to understanding this domain\n    \n    RELATION TYPE GRANULARITY PRINCIPLES:\n    \n    1. **Semantic Precision**: Relations should capture meaningful distinctions\n       - Too vague: \"related_to\", \"associated_with\" (not informative)\n       - Too specific: \"works_at_as_senior_engineer_since_2020\" (overfitting)\n       - Just right: \"works_at\", \"employed_by\", \"manages\" (clear semantics)\n    \n    2. **Directionality Matters**: Choose direction that reflects real-world semantics\n       - \"Person works_at Organization\" (natural)\n       - Not \"Organization employs Person\" (unless that's the focus)\n       - Be consistent in your directional choices\n    \n    3. **Relationship Patterns**: Include relations that appear or could appear multiple times\n       - Medical: \"treats\", \"prescribes\", \"diagnoses\", \"hospitalized_at\"\n       - Business: \"invests_in\", \"founded\", \"acquired_by\", \"partners_with\"\n       - Focus on the relationships that matter for this domain\n    \n    4. **Avoid Redundancy**: Don't create near-synonyms unless there's semantic distinction\n       - Don't have both \"works_at\" and \"employed_by\" (pick one)\n       - Don't have both \"located_in\" and \"based_in\" (pick one)\n       - DO have \"founded\" and \"acquired\" (different events with different meanings)\n    \n    5. **Completeness**: Capture the key relationship types in the domain\n       - Think: \"What questions would someone ask about this data?\"\n       - Include relations that support those analytical questions\n       - Every relation should serve a clear analytical purpose\n    \n    SCOPE-SPECIFIC GUIDANCE:\n    \n    **If scope is \"minimal\":**\n    - Create only the most essential entity types (core concepts only)\n    - Focus on the most frequent and important relationships\n    - Err on the side of broader categories that combine similar concepts\n    - Typical result: 3-8 entity types, 4-10 relation types\n    - Use case: Quick extraction, simple queries, identifying broad patterns\n    - Example: For a business article → Person, Organization, Location, works_at, located_in\n    \n    **If scope is \"balanced\":**\n    - Include entity types for all significant concepts in the domain\n    - Capture the main relationship patterns without over-specifying\n    - Balance specificity with reusability across similar texts\n    - Typical result: 6-12 entity types, 8-15 relation types\n    - Use case: Standard analysis, general-purpose extraction, most common scenarios\n    - Example: For a business article → Person, Organization, Location, Product, Investment, works_at, located_in, founded, invests_in, develops\n    \n    **If scope is \"comprehensive\":**\n    - Include entity types for all distinct and meaningful concepts\n    - Capture nuanced relationship types that reflect domain expertise\n    - Allow for more domain-specific and specialized types\n    - Typical result: 10-20 entity types, 12-25 relation types\n    - Use case: Detailed analysis, domain expertise, research, specialized queries\n    - Example: For a business article → Person, Organization, Location, Product, Investment, Technology, Market, Role, works_at, located_in, founded, invests_in, develops, competes_with, targets, acquired_by, partners_with\n    \n    QUALITY OVER QUANTITY:\n    - It's better to have fewer, well-defined types than many poorly-defined ones\n    - Each type should have a clear, distinct purpose and detailed description\n    - Each type should be distinguishable from others in meaningful ways\n    - Every type you create should be useful for analysis and querying\n    - The numbers above are guidelines, not requirements - use your judgment\n    \n    SELF-CHECK QUESTIONS:\n    Before finalizing your ontology, ask yourself:\n    1. Can I clearly explain when to use each entity type vs. others?\n    2. Would two different people classify entities consistently with these types?\n    3. Are my relation types capturing distinct semantic relationships?\n    4. Can I extract useful, actionable information with this ontology?\n    5. Is this ontology reusable for similar texts in this domain?\n    6. Does the granularity match the requested scope level?\n    \n    ONTOLOGY DESIGN PRINCIPLES:\n    - Entity types should be nouns representing concrete or abstract concepts\n    - Relation types should be verbs or verb phrases that connect entities\n    - Each relation type must specify its domain (subject entity type) and range (object entity type)\n    - Prefer commonly understood terms over jargon unless domain-specific terms are essential\n    - Think about what would be useful for querying and analyzing this type of text\n    - Ensure descriptions are clear and detailed enough for accurate extraction\n    \n    CUSTOM ATTRIBUTES FOR ENTITY TYPES:\n    \n    For each entity type, you should define custom attributes that capture type-specific information:\n    \n    1. **Identify Key Attributes**: What information is important for this entity type?\n       - For Person: birth_date, nationality, occupation\n       - For Organization: founded_date, headquarters_location, industry\n       - For Campaign: launch_date, end_date, budget, target_audience\n       - For Product: release_date, version, price\n    \n    2. **Attribute Types**: Use appropriate data types\n       - \"string\": General text (names, descriptions, IDs)\n       - \"int\": Whole numbers (counts, quantities, years)\n       - \"float\": Decimal numbers (prices, percentages, measurements)\n       - \"bool\": True/false values (is_active, is_public)\n       - \"date\": ISO dates (YYYY-MM-DD format)\n       - Nested objects: For complex structured data (use JSON schema description)\n    \n    3. **Domain-Relevant Attributes**: Focus on what matters for this domain\n       - Medical domain: diagnosis_date, treatment_status, dosage\n       - Business domain: funding_amount, employee_count, revenue\n       - Academic domain: publication_date, citation_count, affiliation\n    \n    4. **Extraction-Friendly**: Choose attributes that can be extracted from text\n       - Prefer attributes commonly mentioned in text\n       - Avoid attributes requiring external knowledge\n       - Balance detail with extractability\n    \n    5. **Scope Considerations**:\n       - Minimal scope: 0-2 attributes per entity type (only essential ones)\n       - Balanced scope: 2-4 attributes per entity type (common, useful attributes)\n       - Comprehensive scope: 4-8 attributes per entity type (detailed characterization)\n    \n    EXAMPLE ENTITY TYPE WITH ATTRIBUTES:\n    \n    ```\n    EntityType: Campaign\n    Description: A marketing or outreach campaign\n    Attributes:\n      - name: campaign_launch_dt\n        type: date\n        description: The date when the campaign was launched\n      - name: campaign_completion_dt\n        type: date\n        description: The date when the campaign ended or is planned to end\n      - name: target_audience\n        type: string\n        description: The intended audience or demographic for the campaign\n      - name: campaign_budget\n        type: float\n        description: The budget allocated for the campaign in dollars\n    ```\n    \n    Your task is to analyze THIS specific text and recommend the most appropriate ontology for it,\n    following the principles above and designing for the requested \"{{ scope }}\" scope level.\n    Include appropriate custom attributes for each entity type based on the domain and text content.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Function to analyze if an existing ontology needs extension for new text\nfunction AnalyzeOntologyExtension(\n  text: string,\n  current_ontology: Ontology,\n  scope: string\n) -> OntologyExtension {\n  client CustomFast\n  prompt #\"\n    You are a knowledge graph ontology expert. Your task is to analyze whether an existing ontology needs to be extended to properly extract knowledge from new text.\n    \n    {{ _.role(\"user\") }}\n    \n    CURRENT ONTOLOGY:\n    \n    Current Entity Types:\n    {% for entity_type in current_ontology.entity_types %}\n    - {{ entity_type.name }}: {{ entity_type.description }}\n      {% if entity_type.attributes|length > 0 %}\n      Attributes:\n      {% for attr in entity_type.attributes %}\n        * {{ attr.name }} ({{ attr.type }}): {{ attr.description }}\n      {% endfor %}\n      {% endif %}\n    {% endfor %}\n    \n    Current Relation Types:\n    {% for relation_type in current_ontology.relation_types %}\n    - {{ relation_type.name }}: {{ relation_type.description }}\n      (Domain: {{ relation_type.domain }}, Range: {{ relation_type.range }})\n    {% endfor %}\n    \n    NEW TEXT TO ANALYZE:\n    {{ text }}\n    \n    ONTOLOGY SCOPE: {{ scope }}\n    \n    YOUR TASK:\n    \n    Analyze whether the current ontology is sufficient to extract critical information from this new text,\n    or whether it needs to be conservatively extended.\n    \n    CONSERVATIVE EXTENSION PRINCIPLES:\n    \n    1. **Default to NO Extension**\n       - Start with the assumption that the existing ontology is sufficient\n       - Only recommend extension if absolutely necessary\n       - Prefer using existing types creatively over adding new ones\n    \n    2. **Critical Information Test**\n       - Extension is ONLY justified if failing to extend would result in losing CRITICAL information\n       - Ask: \"Can the essential facts be captured with existing types?\"\n       - If yes → No extension needed\n       - If no → Consider extension\n    \n    3. **Definition of Critical Information**\n       Critical information is information that:\n       - Is central to understanding the text's main points\n       - Cannot be reasonably represented with existing entity/relation types\n       - Would create significant semantic loss if omitted or forced into existing types\n       - Is not a minor detail or edge case\n    \n    4. **Examples of When Extension IS Needed**\n       - New text discusses \"Chemical Compounds\" but ontology only has \"Substance\"\n         → If chemical-specific relationships matter, extend\n       - New text has \"Software\" entities but ontology only has \"Product\"\n         → If software-specific attributes/relations are critical, extend\n       - New text introduces entirely new domain (e.g., legal domain in a medical ontology)\n         → If domain is central to the text, extend\n    \n    5. **Examples of When Extension is NOT Needed**\n       - New text mentions \"tablets\" but ontology has \"Device\" → Use Device\n       - New text has \"acquired_by\" but ontology has \"purchased_by\" → Use existing similar relation\n       - New text mentions a rare entity type appearing once → Don't extend for one-offs\n       - Differences are purely syntactic (naming) not semantic → Don't extend\n    \n    6. **Backward Compatibility**\n       Any new types you recommend must:\n       - Be distinct from existing types (not redundant or overlapping)\n       - Follow the same naming and description conventions\n       - Work alongside existing types without conflicts\n       - Be at the appropriate granularity level for the scope\n    \n    7. **Scope Awareness**\n       Consider the current scope ({{ scope }}):\n       - For \"minimal\" scope: Be VERY conservative, only extend for major gaps\n       - For \"balanced\" scope: Extend if missing significant concepts\n       - For \"comprehensive\" scope: More willing to add nuanced types\n    \n    ANALYSIS PROCESS:\n    \n    Step 1: Identify concepts in the new text\n    Step 2: For each concept, try to map it to existing entity types\n    Step 3: For each relationship, try to map it to existing relation types\n    Step 4: Identify any critical information that CANNOT be captured\n    Step 5: If critical gaps exist, propose minimal extensions\n    Step 6: Justify why each extension is necessary\n    \n    DECISION CRITERIA:\n    \n    Set needs_extension = true ONLY if:\n    ✓ There are concepts in the text that are critically important\n    ✓ These concepts cannot be reasonably mapped to existing types\n    ✓ Omitting them would cause significant information loss\n    ✓ The extensions align with the current scope level\n    \n    Set needs_extension = false if:\n    ✓ All critical information can be captured with existing types\n    ✓ Minor details might be lost but core facts are preserved\n    ✓ Existing types can be used flexibly to cover new concepts\n    \n    OUTPUT REQUIREMENTS:\n    \n    - If needs_extension = false:\n      * Set new_entity_types and new_relation_types to empty arrays\n      * Explain in reasoning how existing types cover the text\n      * Be specific about which existing types map to which concepts\n    \n    - If needs_extension = true:\n      * Provide ONLY the new types needed (don't repeat existing ones)\n      * Describe in critical_information_at_risk what would be lost\n      * Explain in reasoning why each new type is essential\n      * Keep extensions minimal - only add what's absolutely necessary\n    \n    QUALITY CHECKS:\n    \n    Before recommending extension, verify:\n    1. Can I extract the main facts with existing types? (If yes → no extension)\n    2. Is the \"missing\" information truly critical? (If no → no extension)\n    3. Are my new types distinct from existing ones? (If not → no extension)\n    4. Am I being conservative? (If not → reconsider)\n    \n    Remember: It's better to slightly stretch existing types than to proliferate unnecessary new types.\n    The ontology should evolve slowly and deliberately, not expand with every new text.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n",
}

def get_baml_files():
    return _file_map