{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Spindle Workflow\n",
    "\n",
    "This notebook demonstrates the complete Spindle pipeline from document ingestion to knowledge graph construction and analytics visualization.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This walkthrough covers:\n",
    "1. Setting up configuration in a new project directory\n",
    "2. Creating sample documents with entity variations\n",
    "3. Ingesting documents into storage\n",
    "4. Iteratively recommending and extending an ontology\n",
    "5. Extracting knowledge graph triples\n",
    "6. Resolving duplicate entities\n",
    "7. Inserting into a persistent knowledge graph\n",
    "8. Viewing analytics from event logs\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ensure dependencies are installed: `uv pip install -e \".[dev]\"`\n",
    "from spindle.observability import get_event_recorder, attach_persistent_observer\n",
    "from spindle.observability.storage import EventLogStore\n",
    "- Set `ANTHROPIC_API_KEY` environment variable for LLM operations\n",
    "- Run cells sequentially; each step builds on the previous one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ANTHROPIC_API_KEY configured\n",
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "# Spindle imports\n",
    "from spindle.configuration import SpindleConfig\n",
    "from spindle.ingestion.service import build_config, run_ingestion\n",
    "from spindle.extraction.extractor import SpindleExtractor\n",
    "from spindle.extraction.recommender import OntologyRecommender\n",
    "from spindle.entity_resolution import EntityResolver, ResolutionConfig, get_duplicate_clusters\n",
    "from spindle.graph_store import GraphStore\n",
    "from spindle.vector_store import ChromaVectorStore, get_default_embedding_function\n",
    "from spindle.analytics import AnalyticsStore\n",
    "from spindle.analytics.views import (\n",
    "    corpus_overview,\n",
    "    ontology_recommendation_metrics,\n",
    "    triple_extraction_metrics,\n",
    "    entity_resolution_metrics,\n",
    ")\n",
    "from spindle.observability import get_event_recorder, attach_persistent_observer\n",
    "from spindle.observability.storage import EventLogStore\n",
    "from spindle.baml_client.types import Triple, Entity, SourceMetadata, CharacterSpan\n",
    "\n",
    "# Check for API key\n",
    "if not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  Warning: ANTHROPIC_API_KEY not set. LLM operations will fail.\")\n",
    "else:\n",
    "    print(\"‚úÖ ANTHROPIC_API_KEY configured\")\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Configuration\n",
    "\n",
    "We'll create a new `example_project` directory and configure Spindle to use it for all storage operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created project directory: c:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\n",
      "‚úÖ Event persistence configured: c:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\logs\\analytics.db\n",
      "\n",
      "Configuration initialized:\n",
      "  Storage root: c:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\n",
      "  Vector store: c:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\vector_store\n",
      "  Graph store: c:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\graph\\graph.db\n",
      "  Document catalog: c:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\catalog\\ingestion.db\n",
      "  Logs directory: c:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\logs\n",
      "  LLM configured: True\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable DEBUG_BAML_COLLECTOR=1\n",
    "os.environ['DEBUG_BAML_COLLECTOR'] = '1'\n",
    "\n",
    "# Create example_project directory\n",
    "BASE_DIR = Path.cwd()\n",
    "PROJECT_DIR = BASE_DIR / \"example_project\"\n",
    "\n",
    "# Clean up if it exists (for fresh runs)\n",
    "if PROJECT_DIR.exists():\n",
    "    shutil.rmtree(PROJECT_DIR)\n",
    "\n",
    "PROJECT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created project directory: {PROJECT_DIR}\")\n",
    "\n",
    "# Initialize SpindleConfig with auto-detected LLM\n",
    "config = SpindleConfig.with_auto_detected_llm(\n",
    "    root=PROJECT_DIR,\n",
    ")\n",
    "\n",
    "# Ensure all directories are created\n",
    "config.storage.ensure_directories()\n",
    "\n",
    "# Set up event persistence for analytics\n",
    "# Events from ontology recommendation, triple extraction, and entity resolution\n",
    "# will be saved to the analytics database\n",
    "analytics_db_path = config.storage.log_dir / \"analytics.db\"\n",
    "event_store = EventLogStore(f\"sqlite:///{analytics_db_path}\")\n",
    "detach_observer = attach_persistent_observer(get_event_recorder(), event_store)\n",
    "\n",
    "print(f\"‚úÖ Event persistence configured: {analytics_db_path}\")\n",
    "\n",
    "\n",
    "print(f\"\\nConfiguration initialized:\")\n",
    "print(f\"  Storage root: {config.storage.root}\")\n",
    "print(f\"  Vector store: {config.storage.vector_store_dir}\")\n",
    "print(f\"  Graph store: {config.storage.graph_store_path}\")\n",
    "print(f\"  Document catalog: {config.storage.catalog_path}\")\n",
    "print(f\"  Logs directory: {config.storage.log_dir}\")\n",
    "print(f\"  LLM configured: {config.llm is not None}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Sample Documents\n",
    "\n",
    "We'll create 3 documents with intentional entity variations to demonstrate entity resolution:\n",
    "- Document 1: \"TechCorp\" company, \"John Smith\" person\n",
    "- Document 2: \"Tech Corp\" (space variation), \"J. Smith\" (abbreviation)  \n",
    "- Document 3: \"TechCorp Inc.\" (legal suffix), \"John Smith\" (full name again)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 3 sample documents:\n",
      "  1. doc1_techcorp_overview.md\n",
      "  2. doc2_techcorp_expansion.md\n",
      "  3. doc3_techcorp_financials.md\n",
      "\n",
      "Entity variations to resolve:\n",
      "  - TechCorp / Tech Corp / TechCorp Inc. (same company)\n",
      "  - John Smith / J. Smith (same person)\n"
     ]
    }
   ],
   "source": [
    "# Create documents directory\n",
    "DOCUMENTS_DIR = PROJECT_DIR / \"documents\"\n",
    "DOCUMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Document 1: TechCorp and John Smith (made longer to create multiple chunks)\n",
    "# Default chunk size is 800 chars with 100 overlap, so we'll make this ~2000 chars\n",
    "doc1_content = \"\"\"\n",
    "# TechCorp Company Overview\n",
    "\n",
    "TechCorp is a leading technology company based in San Francisco, California. \n",
    "Founded in 2010, the company specializes in cloud infrastructure and AI solutions.\n",
    "The company has grown rapidly over the past decade, establishing itself as a key \n",
    "player in the enterprise software market. TechCorp's innovative approach to cloud \n",
    "computing has attracted numerous Fortune 500 clients seeking scalable and secure \n",
    "infrastructure solutions.\n",
    "\n",
    "John Smith serves as the Chief Technology Officer at TechCorp. He has been with \n",
    "the company since 2015 and leads the engineering team. John Smith holds a PhD in \n",
    "Computer Science from Stanford University. Under his leadership, TechCorp has \n",
    "developed several breakthrough products that have revolutionized how businesses \n",
    "manage their cloud infrastructure. His expertise in distributed systems and \n",
    "machine learning has been instrumental in the company's success.\n",
    "\n",
    "The company's headquarters are located in San Francisco, and it employs over 500 \n",
    "people worldwide. TechCorp has partnerships with major cloud providers including \n",
    "AWS and Microsoft Azure. These strategic partnerships enable TechCorp to offer \n",
    "comprehensive solutions that integrate seamlessly with existing enterprise \n",
    "infrastructure. The company's commitment to innovation and customer satisfaction \n",
    "has earned it numerous industry awards and recognition from leading technology \n",
    "publications.\n",
    "\n",
    "TechCorp's product portfolio includes enterprise-grade cloud management platforms, \n",
    "AI-powered analytics tools, and security solutions designed for modern businesses. \n",
    "The company continues to invest heavily in research and development, with plans \n",
    "to expand into new markets and develop cutting-edge technologies that will shape \n",
    "the future of cloud computing.\n",
    "\"\"\"\n",
    "\n",
    "# Document 2: Tech Corp (space variation) and J. Smith (abbreviation)\n",
    "doc2_content = \"\"\"\n",
    "# Tech Corp Expansion Plans\n",
    "\n",
    "Tech Corp announced plans to expand its operations to New York City. The expansion \n",
    "will create 200 new jobs in the metropolitan area.\n",
    "\n",
    "J. Smith, CTO of Tech Corp, commented on the expansion: \"This move represents our \n",
    "commitment to growing our presence on the East Coast. We're excited about the \n",
    "talent pool available in New York.\"\n",
    "\n",
    "The New York office will focus on sales and customer support, complementing the \n",
    "engineering work done at the San Francisco headquarters. Tech Corp expects to \n",
    "complete the move by Q2 2025.\n",
    "\"\"\"\n",
    "\n",
    "# Document 3: TechCorp Inc. (legal suffix) and John Smith (full name)\n",
    "doc3_content = \"\"\"\n",
    "# TechCorp Inc. Financial Results\n",
    "\n",
    "TechCorp Inc. reported strong financial results for fiscal year 2024. Revenue \n",
    "grew by 35% year-over-year, reaching $150 million.\n",
    "\n",
    "John Smith, the Chief Technology Officer, attributed the growth to successful \n",
    "product launches and strategic partnerships. \"Our cloud infrastructure platform \n",
    "has seen tremendous adoption,\" said John Smith.\n",
    "\n",
    "The company's board of directors approved a $50 million investment in R&D for \n",
    "2025. TechCorp Inc. plans to hire 100 additional engineers, with positions \n",
    "available in both San Francisco and New York offices.\n",
    "\"\"\"\n",
    "\n",
    "# Write documents\n",
    "doc1_path = DOCUMENTS_DIR / \"doc1_techcorp_overview.md\"\n",
    "doc2_path = DOCUMENTS_DIR / \"doc2_techcorp_expansion.md\"\n",
    "doc3_path = DOCUMENTS_DIR / \"doc3_techcorp_financials.md\"\n",
    "\n",
    "doc1_path.write_text(doc1_content.strip(), encoding=\"utf-8\")\n",
    "doc2_path.write_text(doc2_content.strip(), encoding=\"utf-8\")\n",
    "doc3_path.write_text(doc3_content.strip(), encoding=\"utf-8\")\n",
    "\n",
    "print(\"‚úÖ Created 3 sample documents:\")\n",
    "print(f\"  1. {doc1_path.name}\")\n",
    "print(f\"  2. {doc2_path.name}\")\n",
    "print(f\"  3. {doc3_path.name}\")\n",
    "print(\"\\nEntity variations to resolve:\")\n",
    "print(\"  - TechCorp / Tech Corp / TechCorp Inc. (same company)\")\n",
    "print(\"  - John Smith / J. Smith (same person)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Document Ingestion\n",
    "\n",
    "Ingest the documents into document storage (catalog) and vector storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting documents...\n",
      "\n",
      "‚úÖ Ingestion complete!\n",
      "  Documents processed: 3\n",
      "  Chunks created: 6\n",
      "  Bytes read: 2,974\n",
      "  Errors: 0\n",
      "\n",
      "Chunk details:\n",
      "  Chunk 1: 80dd3827... (doc: 9202ddce...)\n",
      "    Preview: # TechCorp Company Overview\n",
      "\n",
      "TechCorp is a leading technology company based in S...\n",
      "  Chunk 2: 09fcd32f... (doc: 9202ddce...)\n",
      "    Preview: John Smith serves as the Chief Technology Officer at TechCorp. He has been with ...\n",
      "  Chunk 3: 7d4c29b0... (doc: 9202ddce...)\n",
      "    Preview: The company's headquarters are located in San Francisco, and it employs over 500...\n",
      "  Chunk 4: f2fbfec0... (doc: 9202ddce...)\n",
      "    Preview: TechCorp's product portfolio includes enterprise-grade cloud management platform...\n",
      "  Chunk 5: 539f2c09... (doc: d70176e4...)\n",
      "    Preview: # Tech Corp Expansion Plans\n",
      "\n",
      "Tech Corp announced plans to expand its operations ...\n",
      "  Chunk 6: b260f733... (doc: bb912dbf...)\n",
      "    Preview: # TechCorp Inc. Financial Results\n",
      "\n",
      "TechCorp Inc. reported strong financial resul...\n"
     ]
    }
   ],
   "source": [
    "# Build ingestion config using SpindleConfig\n",
    "ingestion_config = build_config(spindle_config=config)\n",
    "\n",
    "# Ingest all documents\n",
    "document_paths = [doc1_path, doc2_path, doc3_path]\n",
    "print(\"Ingesting documents...\")\n",
    "ingestion_result = run_ingestion(document_paths, ingestion_config)\n",
    "\n",
    "print(f\"\\n‚úÖ Ingestion complete!\")\n",
    "print(f\"  Documents processed: {ingestion_result.metrics.processed_documents}\")\n",
    "print(f\"  Chunks created: {ingestion_result.metrics.processed_chunks}\")\n",
    "print(f\"  Bytes read: {ingestion_result.metrics.bytes_read:,}\")\n",
    "print(f\"  Errors: {len(ingestion_result.metrics.errors)}\")\n",
    "\n",
    "# Display chunk information\n",
    "print(f\"\\nChunk details:\")\n",
    "for i, chunk in enumerate(ingestion_result.chunks, 1):\n",
    "    print(f\"  Chunk {i}: {chunk.chunk_id[:8]}... (doc: {chunk.document_id[:8]}...)\")\n",
    "    print(f\"    Preview: {chunk.text[:80]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Iterative Ontology Recommendation\n",
    "\n",
    "We'll recommend an ontology from the first document, then iteratively extend it as we analyze the remaining documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Recommending initial ontology from Document 1...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "=== DEBUG: Collector attributes ===\n",
      "Collector type: <class 'baml_py.baml_py.Collector'>\n",
      "Collector dir: ['clear', 'id', 'last', 'logs', 'usage']\n",
      "Number of logs: 1\n",
      "Log type: <class 'baml_py.baml_py.FunctionLog'>\n",
      "Log dir: ['calls', 'function_name', 'id', 'log_type', 'metadata', 'raw_llm_response', 'selected_call', 'tags', 'timing', 'usage']\n",
      "\n",
      "selected_call type: <class 'baml_py.baml_py.LLMCall'>\n",
      "selected_call dir: ['client_name', 'http_request', 'http_response', 'provider', 'selected', 'timing', 'usage']\n",
      "selected_call.client_name: CustomHaiku\n",
      "selected_call.provider: anthropic\n",
      "\n",
      "http_response type: <class 'baml_py.baml_py.HTTPResponse'>\n",
      "http_response dir: ['body', 'headers', 'status']\n",
      "http_response.body type: <class 'baml_py.baml_py.HTTPBody'>\n",
      "\n",
      "usage type: <class 'baml_py.baml_py.Usage'>\n",
      "usage dir: ['cached_input_tokens', 'input_tokens', 'output_tokens']\n",
      "\n",
      "metadata type: <class 'dict'>\n",
      "metadata keys: ['baml.runtime', 'baml.language']\n",
      "========================================\n",
      "Initial ontology:\n",
      "  Entity types: 5\n",
      "    - Organization: A technology company or business entity\n",
      "    - Person: An individual associated with the organization\n",
      "    - Product: Technology solutions or services offered by the organization\n",
      "    - Location: Geographic places associated with the organization\n",
      "    - Partnership: Strategic business relationships between organizations\n",
      "  Relation types: 5\n",
      "    - works_at: Person ‚Üí Organization\n",
      "    - leads: Person ‚Üí Organization\n",
      "    - located_in: Organization ‚Üí Location\n",
      "    - develops: Organization ‚Üí Product\n",
      "    - partners_with: Organization ‚Üí Organization\n"
     ]
    }
   ],
   "source": [
    "# Create recommender using config\n",
    "recommender = config.create_recommender()\n",
    "\n",
    "# Get document texts\n",
    "doc1_text = doc1_path.read_text()\n",
    "doc2_text = doc2_path.read_text()\n",
    "doc3_text = doc3_path.read_text()\n",
    "\n",
    "# Step 1: Recommend initial ontology from Document 1\n",
    "print(\"Step 1: Recommending initial ontology from Document 1...\")\n",
    "print(\"-\" * 70)\n",
    "rec1 = recommender.recommend(text=doc1_text, scope=\"balanced\")\n",
    "\n",
    "print(f\"Initial ontology:\")\n",
    "print(f\"  Entity types: {len(rec1.ontology.entity_types)}\")\n",
    "for et in rec1.ontology.entity_types:\n",
    "    print(f\"    - {et.name}: {et.description}\")\n",
    "print(f\"  Relation types: {len(rec1.ontology.relation_types)}\")\n",
    "for rt in rec1.ontology.relation_types:\n",
    "    print(f\"    - {rt.name}: {rt.domain} ‚Üí {rt.range}\")\n",
    "\n",
    "current_ontology = rec1.ontology\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Analyzing Document 2 for ontology extension...\n",
      "----------------------------------------------------------------------\n",
      "Extension needed: True\n",
      "  New entity types: 1\n",
      "    - Office: A discrete organizational unit or branch location of an Organization (e.g., regional office, sales office, support center). Used to represent planned or existing offices with their specific operational focus, staffing plans, and timelines.\n",
      "  New relation types: 2\n",
      "    - has_office: Organization ‚Üí Office\n",
      "    - office_located_in: Office ‚Üí Location\n",
      "\n",
      "Reasoning: Step 1 ‚Äî Concepts in the text: organization (Tech Corp), location (New York City, San Francisco), a planned/new office (New York office), job creation count (200 new jobs), office functional focus (sa...\n",
      "\n",
      "‚úÖ Extended ontology now has:\n",
      "  Entity types: 5\n",
      "  Relation types: 7\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Analyze Document 2 for extension needs\n",
    "print(\"\\nStep 2: Analyzing Document 2 for ontology extension...\")\n",
    "print(\"-\" * 70)\n",
    "extension2 = recommender.analyze_extension(\n",
    "    text=doc2_text,\n",
    "    current_ontology=current_ontology,\n",
    "    scope=\"balanced\"\n",
    ")\n",
    "\n",
    "print(f\"Extension needed: {extension2.needs_extension}\")\n",
    "if extension2.needs_extension:\n",
    "    print(f\"  New entity types: {len(extension2.new_entity_types)}\")\n",
    "    for et in extension2.new_entity_types:\n",
    "        print(f\"    - {et.name}: {et.description}\")\n",
    "    print(f\"  New relation types: {len(extension2.new_relation_types)}\")\n",
    "    for rt in extension2.new_relation_types:\n",
    "        print(f\"    - {rt.name}: {rt.domain} ‚Üí {rt.range}\")\n",
    "    print(f\"\\nReasoning: {extension2.reasoning[:200]}...\")\n",
    "    \n",
    "    # Extend ontology\n",
    "    current_ontology = recommender.extend_ontology(current_ontology, extension2)\n",
    "    print(f\"\\n‚úÖ Extended ontology now has:\")\n",
    "    print(f\"  Entity types: {len(current_ontology.entity_types)}\")\n",
    "    print(f\"  Relation types: {len(current_ontology.relation_types)}\")\n",
    "else:\n",
    "    print(\"  No extension needed - existing ontology covers Document 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Analyzing Document 3 for ontology extension...\n",
      "----------------------------------------------------------------------\n",
      "Extension needed: False\n",
      "  No extension needed - existing ontology covers Document 3\n",
      "\n",
      "üìä Final ontology summary:\n",
      "  Total entity types: 5\n",
      "  Total relation types: 7\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Analyze Document 3 for extension needs\n",
    "print(\"\\nStep 3: Analyzing Document 3 for ontology extension...\")\n",
    "print(\"-\" * 70)\n",
    "extension3 = recommender.analyze_extension(\n",
    "    text=doc3_text,\n",
    "    current_ontology=current_ontology,\n",
    "    scope=\"balanced\"\n",
    ")\n",
    "\n",
    "print(f\"Extension needed: {extension3.needs_extension}\")\n",
    "if extension3.needs_extension:\n",
    "    print(f\"  New entity types: {len(extension3.new_entity_types)}\")\n",
    "    for et in extension3.new_entity_types:\n",
    "        print(f\"    - {et.name}: {et.description}\")\n",
    "    print(f\"  New relation types: {len(extension3.new_relation_types)}\")\n",
    "    for rt in extension3.new_relation_types:\n",
    "        print(f\"    - {rt.name}: {rt.domain} ‚Üí {rt.range}\")\n",
    "    print(f\"\\nReasoning: {extension3.reasoning[:200]}...\")\n",
    "    \n",
    "    # Extend ontology\n",
    "    current_ontology = recommender.extend_ontology(current_ontology, extension3)\n",
    "    print(f\"\\n‚úÖ Extended ontology now has:\")\n",
    "    print(f\"  Entity types: {len(current_ontology.entity_types)}\")\n",
    "    print(f\"  Relation types: {len(current_ontology.relation_types)}\")\n",
    "else:\n",
    "    print(\"  No extension needed - existing ontology covers Document 3\")\n",
    "\n",
    "print(f\"\\nüìä Final ontology summary:\")\n",
    "print(f\"  Total entity types: {len(current_ontology.entity_types)}\")\n",
    "print(f\"  Total relation types: {len(current_ontology.relation_types)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Triple Extraction\n",
    "\n",
    "Extract knowledge graph triples from all documents using the final ontology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting triples from Document 1...\n",
      "\n",
      "=== DEBUG: Collector attributes ===\n",
      "Collector type: <class 'baml_py.baml_py.Collector'>\n",
      "Collector dir: ['clear', 'id', 'last', 'logs', 'usage']\n",
      "Number of logs: 1\n",
      "Log type: <class 'baml_py.baml_py.FunctionLog'>\n",
      "Log dir: ['calls', 'function_name', 'id', 'log_type', 'metadata', 'raw_llm_response', 'selected_call', 'tags', 'timing', 'usage']\n",
      "\n",
      "raw_llm_response type: <class 'str'>\n",
      "raw_llm_response preview (first 500 chars): {\n",
      "  \"triples\": [\n",
      "    {\n",
      "      \"subject\": {\n",
      "        \"name\": \"TechCorp\",\n",
      "        \"type\": \"Organization\",\n",
      "        \"description\": \"TechCorp is a technology company specializing in cloud infrastructure and AI solutions, positioned as a key player in the enterprise software market.\",\n",
      "        \"custom_atts\": {\n",
      "          \"founded_date\": {\n",
      "            \"value\": \"2010-01-01\",\n",
      "            \"type\": \"date\"\n",
      "          },\n",
      "          \"headquarters_location\": {\n",
      "            \"value\": \"San Francisco, California\",\n",
      "       \n",
      "Parsed response has keys: ['triples', 'reasoning']\n",
      "========================================\n",
      "  Extracted 10 triples\n",
      "\n",
      "Extracting triples from Document 2...\n",
      "\n",
      "=== DEBUG: Collector attributes ===\n",
      "Collector type: <class 'baml_py.baml_py.Collector'>\n",
      "Collector dir: ['clear', 'id', 'last', 'logs', 'usage']\n",
      "Number of logs: 1\n",
      "Log type: <class 'baml_py.baml_py.FunctionLog'>\n",
      "Log dir: ['calls', 'function_name', 'id', 'log_type', 'metadata', 'raw_llm_response', 'selected_call', 'tags', 'timing', 'usage']\n",
      "\n",
      "raw_llm_response type: <class 'str'>\n",
      "raw_llm_response preview (first 500 chars): {\n",
      "    \"triples\": [\n",
      "        {\n",
      "            \"subject\": {\n",
      "                \"name\": \"TechCorp\",\n",
      "                \"type\": \"Organization\",\n",
      "                \"description\": \"A technology company planning expansion to New York City\",\n",
      "                \"custom_atts\": {\n",
      "                    \"headquarters_location\": {\"value\": \"San Francisco, California\", \"type\": \"string\"},\n",
      "                    \"industry\": {\"value\": null, \"type\": \"string\"},\n",
      "                    \"founded_date\": {\"value\": null, \"type\": \"date\"},\n",
      "       \n",
      "Parsed response has keys: ['triples', 'reasoning']\n",
      "========================================\n",
      "  Extracted 3 triples\n",
      "\n",
      "Extracting triples from Document 3...\n",
      "\n",
      "=== DEBUG: Collector attributes ===\n",
      "Collector type: <class 'baml_py.baml_py.Collector'>\n",
      "Collector dir: ['clear', 'id', 'last', 'logs', 'usage']\n",
      "Number of logs: 1\n",
      "Log type: <class 'baml_py.baml_py.FunctionLog'>\n",
      "Log dir: ['calls', 'function_name', 'id', 'log_type', 'metadata', 'raw_llm_response', 'selected_call', 'tags', 'timing', 'usage']\n",
      "\n",
      "raw_llm_response type: <class 'str'>\n",
      "raw_llm_response preview (first 500 chars): {\n",
      "  \"triples\": [\n",
      "    {\n",
      "      \"subject\": {\n",
      "        \"name\": \"TechCorp\",\n",
      "        \"type\": \"Organization\",\n",
      "        \"description\": \"TechCorp Inc. reported strong financial results for fiscal year 2024, with revenue growth and planned R&D investment and hiring for 2025.\",\n",
      "        \"custom_atts\": {\n",
      "          \"founded_date\": {\n",
      "            \"value\": null,\n",
      "            \"type\": \"date\"\n",
      "          },\n",
      "          \"headquarters_location\": {\n",
      "            \"value\": null,\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "        \n",
      "Parsed response has keys: ['triples', 'reasoning']\n",
      "========================================\n",
      "  Extracted 7 triples\n",
      "\n",
      "‚úÖ Total triples extracted: 20\n",
      "\n",
      "Sample triples:\n",
      "\n",
      "  1. TechCorp (Organization)\n",
      "     --[located_in]-->\n",
      "     San Francisco, California (Location)\n",
      "     Evidence: \"TechCorp is a leading technology company based in San Franci...\"\n",
      "\n",
      "  2. TechCorp (Organization)\n",
      "     --[has_office]-->\n",
      "     TechCorp Headquarters (Office)\n",
      "     Evidence: \"The company's headquarters are located in San Francisco,...\"\n",
      "\n",
      "  3. TechCorp Headquarters (Office)\n",
      "     --[office_located_in]-->\n",
      "     San Francisco, California (Location)\n",
      "     Evidence: \"The company's headquarters are located in San Francisco,...\"\n",
      "\n",
      "  4. John Smith (Person)\n",
      "     --[works_at]-->\n",
      "     TechCorp (Organization)\n",
      "     Evidence: \"John Smith serves as the Chief Technology Officer at TechCor...\"\n",
      "\n",
      "  5. John Smith (Person)\n",
      "     --[leads]-->\n",
      "     TechCorp (Organization)\n",
      "     Evidence: \"He has been with the company since 2015 and leads the engine...\"\n",
      "\n",
      "\n",
      "Adding manual triples with entity name variations for resolution testing...\n",
      "  Added 3 manual triples with entity variations:\n",
      "    - TechCorp Industries (should resolve to TechCorp)\n",
      "    - Dr. John Smith (should resolve to John Smith)\n",
      "  Total triples now: 23\n"
     ]
    }
   ],
   "source": [
    "# Create extractor with final ontology\n",
    "extractor = config.create_extractor(ontology=current_ontology)\n",
    "\n",
    "# Extract triples from all documents, maintaining entity consistency\n",
    "all_triples = []\n",
    "\n",
    "print(\"Extracting triples from Document 1...\")\n",
    "result1 = extractor.extract(\n",
    "    text=doc1_text,\n",
    "    source_name=\"doc1_techcorp_overview.md\",\n",
    "    source_url=str(doc1_path)\n",
    ")\n",
    "all_triples.extend(result1.triples)\n",
    "print(f\"  Extracted {len(result1.triples)} triples\")\n",
    "\n",
    "print(\"\\nExtracting triples from Document 2...\")\n",
    "result2 = extractor.extract(\n",
    "    text=doc2_text,\n",
    "    source_name=\"doc2_techcorp_expansion.md\",\n",
    "    source_url=str(doc2_path),\n",
    "    existing_triples=all_triples  # Maintain entity consistency\n",
    ")\n",
    "all_triples.extend(result2.triples)\n",
    "print(f\"  Extracted {len(result2.triples)} triples\")\n",
    "\n",
    "print(\"\\nExtracting triples from Document 3...\")\n",
    "result3 = extractor.extract(\n",
    "    text=doc3_text,\n",
    "    source_name=\"doc3_techcorp_financials.md\",\n",
    "    source_url=str(doc3_path),\n",
    "    existing_triples=all_triples  # Maintain entity consistency\n",
    ")\n",
    "all_triples.extend(result3.triples)\n",
    "print(f\"  Extracted {len(result3.triples)} triples\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total triples extracted: {len(all_triples)}\")\n",
    "\n",
    "# Display sample triples\n",
    "print(\"\\nSample triples:\")\n",
    "for i, triple in enumerate(all_triples[:5], 1):\n",
    "    print(f\"\\n  {i}. {triple.subject.name} ({triple.subject.type})\")\n",
    "    print(f\"     --[{triple.predicate}]-->\")\n",
    "    print(f\"     {triple.object.name} ({triple.object.type})\")\n",
    "    if triple.supporting_spans:\n",
    "        print(f\"     Evidence: \\\"{triple.supporting_spans[0].text[:60]}...\\\"\")\n",
    "\n",
    "# Add manual triples with entity name variations to test entity resolution\n",
    "# These refer to existing entities but use different names\n",
    "print(\"\\n\\nAdding manual triples with entity name variations for resolution testing...\")\n",
    "\n",
    "# Find existing entities to create variations for\n",
    "existing_company = None\n",
    "existing_person = None\n",
    "for triple in all_triples:\n",
    "    if triple.subject.type == \"Organization\" and existing_company is None:\n",
    "        existing_company = triple.subject\n",
    "    if triple.subject.type == \"Person\" and existing_person is None:\n",
    "        existing_person = triple.subject\n",
    "    if triple.object.type == \"Organization\" and existing_company is None:\n",
    "        existing_company = triple.object\n",
    "    if triple.object.type == \"Person\" and existing_person is None:\n",
    "        existing_person = triple.object\n",
    "    if existing_company and existing_person:\n",
    "        break\n",
    "\n",
    "if existing_company and existing_person:\n",
    "    # Create triples with name variations\n",
    "    # Variation 1: \"TechCorp Industries\" (should resolve to \"TechCorp\")\n",
    "    techcorp_variation = Entity(\n",
    "        name=\"TechCorp Industries\",\n",
    "        type=existing_company.type,\n",
    "        description=existing_company.description,\n",
    "        custom_atts={}\n",
    "    )\n",
    "    \n",
    "    # Variation 2: \"Dr. John Smith\" (should resolve to \"John Smith\")\n",
    "    john_variation = Entity(\n",
    "        name=\"Dr. John Smith\",\n",
    "        type=existing_person.type,\n",
    "        description=existing_person.description,\n",
    "        custom_atts={}\n",
    "    )\n",
    "    \n",
    "    # Create manual triples\n",
    "    manual_triple1 = Triple(\n",
    "        subject=techcorp_variation,\n",
    "        predicate=\"located_in\",\n",
    "        object=Entity(\n",
    "            name=\"San Francisco Bay Area\",\n",
    "            type=\"Location\",\n",
    "            description=\"Geographical region\",\n",
    "            custom_atts={}\n",
    "        ),\n",
    "        source=SourceMetadata(\n",
    "            source_name=\"manual_addition.md\",\n",
    "            source_url=str(PROJECT_DIR / \"manual_addition.md\")\n",
    "        ),\n",
    "        supporting_spans=[CharacterSpan(text=\"TechCorp Industries is located in the San Francisco Bay Area\")],\n",
    "        extraction_datetime=datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    )\n",
    "    \n",
    "    manual_triple2 = Triple(\n",
    "        subject=john_variation,\n",
    "        predicate=\"works_at\",\n",
    "        object=techcorp_variation,\n",
    "        source=SourceMetadata(\n",
    "            source_name=\"manual_addition.md\",\n",
    "            source_url=str(PROJECT_DIR / \"manual_addition.md\")\n",
    "        ),\n",
    "        supporting_spans=[CharacterSpan(text=\"Dr. John Smith works at TechCorp Industries\")],\n",
    "        extraction_datetime=datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    )\n",
    "    \n",
    "    manual_triple3 = Triple(\n",
    "        subject=Entity(\n",
    "            name=\"TechCorp Industries\",\n",
    "            type=\"Organization\",\n",
    "            description=\"Technology company\",\n",
    "            custom_atts={}\n",
    "        ),\n",
    "        predicate=\"partners_with\",\n",
    "        object=Entity(\n",
    "            name=\"Google Cloud\",\n",
    "            type=\"Organization\",\n",
    "            description=\"Cloud provider\",\n",
    "            custom_atts={}\n",
    "        ),\n",
    "        source=SourceMetadata(\n",
    "            source_name=\"manual_addition.md\",\n",
    "            source_url=str(PROJECT_DIR / \"manual_addition.md\")\n",
    "        ),\n",
    "        supporting_spans=[CharacterSpan(text=\"TechCorp Industries partners with Google Cloud\")],\n",
    "        extraction_datetime=datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    )\n",
    "    \n",
    "    all_triples.extend([manual_triple1, manual_triple2, manual_triple3])\n",
    "    print(f\"  Added 3 manual triples with entity variations:\")\n",
    "    print(f\"    - TechCorp Industries (should resolve to TechCorp)\")\n",
    "    print(f\"    - Dr. John Smith (should resolve to John Smith)\")\n",
    "    print(f\"  Total triples now: {len(all_triples)}\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  Could not find existing entities to create variations for\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Entity Resolution\n",
    "\n",
    "Identify and link duplicate entities using semantic blocking and matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading triples into temporary graph store...\n",
      "  Added 15 triples\n",
      "  Initial nodes: 35\n",
      "\n",
      "Running entity resolution...\n",
      "\n",
      "‚úÖ Resolution complete!\n",
      "  Nodes processed: 35\n",
      "  Edges processed: 85\n",
      "  Blocks created: 23\n",
      "  SAME_AS edges created: 52\n",
      "  Duplicate clusters: 23\n",
      "  Execution time: 277.83s\n"
     ]
    }
   ],
   "source": [
    "# Initialize stores for entity resolution\n",
    "# First, create a temporary graph store to load triples\n",
    "temp_graph_name = \"e2e_temp_resolution\"\n",
    "temp_store = GraphStore(temp_graph_name, config=config)\n",
    "temp_store.create_graph(temp_graph_name)\n",
    "\n",
    "# Add triples to temporary graph\n",
    "print(\"Loading triples into temporary graph store...\")\n",
    "triples_added = temp_store.add_triples(all_triples)\n",
    "print(f\"  Added {triples_added} triples\")\n",
    "\n",
    "# Get initial node count\n",
    "initial_nodes = temp_store.nodes()\n",
    "print(f\"  Initial nodes: {len(initial_nodes)}\")\n",
    "\n",
    "# Initialize vector store for embeddings\n",
    "vector_store = ChromaVectorStore(\n",
    "    collection_name=\"e2e_resolution_embeddings\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Configure resolution\n",
    "resolution_config = ResolutionConfig(\n",
    "    blocking_threshold=0.85,\n",
    "    matching_threshold=0.75,\n",
    "    clustering_method=\"hierarchical\",\n",
    "    batch_size=20,\n",
    ")\n",
    "\n",
    "# Create resolver\n",
    "resolver = EntityResolver(config=resolution_config)\n",
    "\n",
    "# Provide domain context\n",
    "context = \"\"\"\n",
    "This is a knowledge graph about technology companies and people.\n",
    "Common variations:\n",
    "- Corp, Corporation, Inc., LLC are equivalent suffixes\n",
    "- Abbreviations like NYC = New York City are common\n",
    "- First name abbreviations (J. = John) are common\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nRunning entity resolution...\")\n",
    "resolution_result = resolver.resolve_entities(\n",
    "    graph_store=temp_store,\n",
    "    vector_store=vector_store,\n",
    "    apply_to_nodes=True,\n",
    "    apply_to_edges=True,\n",
    "    context=context\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Resolution complete!\")\n",
    "print(f\"  Nodes processed: {resolution_result.total_nodes_processed}\")\n",
    "print(f\"  Edges processed: {resolution_result.total_edges_processed}\")\n",
    "print(f\"  Blocks created: {resolution_result.blocks_created}\")\n",
    "print(f\"  SAME_AS edges created: {resolution_result.same_as_edges_created}\")\n",
    "print(f\"  Duplicate clusters: {resolution_result.duplicate_clusters}\")\n",
    "print(f\"  Execution time: {resolution_result.execution_time_seconds:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node matches found:\n",
      "\n",
      "  Match 1:\n",
      "    Entity 1: CHIEF TECHNOLOGY OFFICER\n",
      "    Entity 2: CHIEF TECHNOLOGY OFFICER AT TECHCORP\n",
      "    Confidence: 0.95\n",
      "    Reasoning: These are clearly the same job role. Both have identical titles ('Chief Technology Officer'), identical start dates (2015-01-01), and nearly identical...\n",
      "\n",
      "  Match 2:\n",
      "    Entity 1: CHIEF TECHNOLOGY OFFICER\n",
      "    Entity 2: CTO, TECHCORP\n",
      "    Confidence: 0.95\n",
      "    Reasoning: These represent the same job role at TechCorp. 'CTO' is a standard abbreviation for 'Chief Technology Officer'. Both are JobRole entities at TechCorp ...\n",
      "\n",
      "  Match 3:\n",
      "    Entity 1: CHIEF TECHNOLOGY OFFICER AT TECHCORP\n",
      "    Entity 2: CTO, TECHCORP\n",
      "    Confidence: 0.95\n",
      "    Reasoning: These are the same role - CTO is the standard abbreviation for Chief Technology Officer. Both are JobRole entities at TechCorp with consistent descrip...\n",
      "\n",
      "  Match 10:\n",
      "    Entity 1: SAN FRANCISCO, CALIFORNIA\n",
      "    Entity 2: SAN FRANCISCO\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Both entities refer to San Francisco, California. The first uses a comma-separated format while the second uses structured attributes, but both have i...\n",
      "\n",
      "  Match 11:\n",
      "    Entity 1: NEW YORK CITY, NEW YORK\n",
      "    Entity 2: NEW YORK CITY\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Both entities clearly refer to New York City. The first uses comma-separated format with state specification, while the second uses a simpler format. ...\n",
      "\n",
      "  Match 12:\n",
      "    Entity 1: NEW YORK, NEW YORK\n",
      "    Entity 2: NEW YORK CITY, NEW YORK\n",
      "    Confidence: 0.75\n",
      "    Reasoning: These likely refer to the same location. 'New York' when referring to a city typically means New York City, especially in business contexts. Both have...\n",
      "\n",
      "  Match 13:\n",
      "    Entity 1: NEW YORK, NEW YORK\n",
      "    Entity 2: NEW YORK CITY\n",
      "    Confidence: 0.75\n",
      "    Reasoning: Similar to the previous match, 'New York' as a city name in business contexts typically refers to New York City. Both descriptions mention TechCorp of...\n",
      "\n",
      "  Match 14:\n",
      "    Entity 1: TECHCORP NEW YORK EXPANSION\n",
      "    Entity 2: TECHCORP EXPANSION INTO NEW YORK CITY\n",
      "    Confidence: 0.95\n",
      "    Reasoning: These entities describe the exact same expansion plan: TechCorp expanding to New York City, creating 200 jobs in sales and customer support, completin...\n",
      "\n",
      "  Match 15:\n",
      "    Entity 1: TECHCORP NEW YORK EXPANSION\n",
      "    Entity 2: TECHCORP NEW YORK EXPANSION HIRING PLAN\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Both describe the same hiring initiative for TechCorp's New York expansion - 200 jobs in sales and customer support, completing by Q2 2025. The Hiring...\n",
      "\n",
      "  Match 16:\n",
      "    Entity 1: TECHCORP NEW YORK EXPANSION\n",
      "    Entity 2: TECHCORP EXPANSION TO NEW YORK CITY\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Identical expansion plans - same company, location (NYC), job count (200), focus areas (sales and customer support), and timeline (Q2 2025). Different...\n",
      "\n",
      "  Match 17:\n",
      "    Entity 1: TECHCORP NEW YORK EXPANSION\n",
      "    Entity 2: TECHCORP NEW YORK EXPANSION (HIRING)\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Same expansion initiative with identical details: 200 jobs, New York City location, sales and customer support focus, Q2 2025 completion. The parenthe...\n",
      "\n",
      "  Match 18:\n",
      "    Entity 1: TECHCORP 2025 ENGINEERING HIRING PLAN\n",
      "    Entity 2: TECHCORP 2025 ENGINEERING HIRING\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Both describe the same engineering hiring initiative: 100 engineers to be hired in 2025 across San Francisco and New York offices. The job count, time...\n",
      "\n",
      "  Match 19:\n",
      "    Entity 1: TECHCORP EXPANSION INTO NEW YORK CITY\n",
      "    Entity 2: TECHCORP NEW YORK EXPANSION HIRING PLAN\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Both describe the same New York expansion creating 200 jobs in sales and customer support, completing by Q2 2025. One focuses on the expansion aspect,...\n",
      "\n",
      "  Match 20:\n",
      "    Entity 1: TECHCORP EXPANSION INTO NEW YORK CITY\n",
      "    Entity 2: TECHCORP EXPANSION TO NEW YORK CITY\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Identical expansion plans with same job count (200), location (NYC), focus areas (sales and customer support), and timeline (Q2 2025). Minor name vari...\n",
      "\n",
      "  Match 21:\n",
      "    Entity 1: TECHCORP EXPANSION INTO NEW YORK CITY\n",
      "    Entity 2: TECHCORP NEW YORK EXPANSION (HIRING)\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Same expansion initiative with identical parameters: 200 jobs, New York City, sales and customer support focus, Q2 2025 completion. Different entity t...\n",
      "\n",
      "  Match 22:\n",
      "    Entity 1: TECHCORP NEW YORK EXPANSION HIRING PLAN\n",
      "    Entity 2: TECHCORP EXPANSION TO NEW YORK CITY\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Both describe the same business initiative - TechCorp's expansion to NYC creating 200 jobs in sales and customer support by Q2 2025. One emphasizes th...\n",
      "\n",
      "  Match 23:\n",
      "    Entity 1: TECHCORP NEW YORK EXPANSION HIRING PLAN\n",
      "    Entity 2: TECHCORP NEW YORK EXPANSION (HIRING)\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Identical hiring plans for the New York expansion - same job count (200), roles (sales and customer support), location (NYC), and timeline (Q2 2025). ...\n",
      "\n",
      "  Match 24:\n",
      "    Entity 1: TECHCORP EXPANSION TO NEW YORK CITY\n",
      "    Entity 2: TECHCORP NEW YORK EXPANSION (HIRING)\n",
      "    Confidence: 0.95\n",
      "    Reasoning: Same expansion plan with identical details across all key attributes: 200 jobs, NYC location, sales and customer support focus, Q2 2025 timeline. Diff...\n",
      "\n",
      "  Match 25:\n",
      "    Entity 1: TECHCORP FY2024 REVENUE\n",
      "    Entity 2: TECHCORP 2024 REVENUE\n",
      "    Confidence: 0.95\n",
      "    Reasoning: These entities are clear duplicates representing the same financial metric. Both have identical descriptions ('Annual revenue for fiscal year 2024, sh...\n",
      "\n",
      "\n",
      "Duplicate clusters: 6\n",
      "  Cluster 1: ['TECHCORP EXPANSION INTO NEW YORK CITY', 'TECHCORP EXPANSION TO NEW YORK CITY', 'TECHCORP NEW YORK EXPANSION', 'TECHCORP NEW YORK EXPANSION (HIRING)', 'TECHCORP NEW YORK EXPANSION HIRING PLAN']\n",
      "  Cluster 2: ['TECHCORP 2025 ENGINEERING HIRING', 'TECHCORP 2025 ENGINEERING HIRING PLAN']\n",
      "  Cluster 3: ['TECHCORP 2024 REVENUE', 'TECHCORP FY2024 REVENUE']\n",
      "  Cluster 4: ['SAN FRANCISCO', 'SAN FRANCISCO, CALIFORNIA']\n",
      "  Cluster 5: ['NEW YORK CITY', 'NEW YORK CITY, NEW YORK', 'NEW YORK, NEW YORK']\n",
      "  Cluster 6: ['CHIEF TECHNOLOGY OFFICER', 'CHIEF TECHNOLOGY OFFICER AT TECHCORP', 'CTO, TECHCORP']\n",
      "\n",
      "\n",
      "SAME_AS edges: 38\n",
      "  SAN FRANCISCO --[SAME_AS]--> SAN FRANCISCO, CALIFORNIA\n",
      "  TECHCORP NEW YORK EXPANSION (HIRING) --[SAME_AS]--> TECHCORP NEW YORK EXPANSION\n",
      "  TECHCORP EXPANSION TO NEW YORK CITY --[SAME_AS]--> TECHCORP NEW YORK EXPANSION\n",
      "  TECHCORP NEW YORK EXPANSION HIRING PLAN --[SAME_AS]--> TECHCORP NEW YORK EXPANSION\n",
      "  TECHCORP EXPANSION INTO NEW YORK CITY --[SAME_AS]--> TECHCORP NEW YORK EXPANSION\n"
     ]
    }
   ],
   "source": [
    "# Display resolution results\n",
    "print(\"\\nNode matches found:\")\n",
    "for i, match in enumerate(resolution_result.node_matches, 1):\n",
    "    if match.is_duplicate:\n",
    "        print(f\"\\n  Match {i}:\")\n",
    "        print(f\"    Entity 1: {match.entity1_id}\")\n",
    "        print(f\"    Entity 2: {match.entity2_id}\")\n",
    "        print(f\"    Confidence: {match.confidence:.2f}\")\n",
    "        print(f\"    Reasoning: {match.reasoning[:150]}...\")\n",
    "\n",
    "# Get duplicate clusters\n",
    "clusters = get_duplicate_clusters(temp_store)\n",
    "print(f\"\\n\\nDuplicate clusters: {len(clusters)}\")\n",
    "for i, cluster in enumerate(clusters, 1):\n",
    "    print(f\"  Cluster {i}: {sorted(cluster)}\")\n",
    "\n",
    "# Query SAME_AS edges\n",
    "same_as_edges = temp_store.query_by_pattern(predicate=\"SAME_AS\")\n",
    "print(f\"\\n\\nSAME_AS edges: {len(same_as_edges)}\")\n",
    "for edge in same_as_edges[:5]:\n",
    "    print(f\"  {edge['subject']} --[SAME_AS]--> {edge['object']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Knowledge Graph Insertion\n",
    "\n",
    "Insert the resolved triples into a persistent knowledge graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting resolved triples into knowledge graph...\n",
      "  Inserted 93 triples\n",
      "\n",
      "‚úÖ Knowledge graph created!\n",
      "  Nodes: 35\n",
      "  Edges: 85\n",
      "  Sources: doc1_techcorp_overview.md, doc2_techcorp_expansion.md, doc3_techcorp_financials.md, entity_resolution, manual_addition.md\n",
      "  Predicates: LOCATED_IN, ROLE_AT, TARGETS_MARKET, ANNOUNCED_ACTION, LEADS, SPECIALIZES_IN, HOLDS_POSITION, HAS_PLAN, APPROVED_INVESTMENT, DEVELOPS...\n",
      "\n",
      "\n",
      "Example queries:\n",
      "\n",
      "1. All 'works_at' relationships:\n",
      "   J. SMITH --[works_at]--> TECHCORP\n",
      "   JOHN SMITH --[works_at]--> TECHCORP\n",
      "   DR. JOHN SMITH --[works_at]--> TECHCORP INDUSTRIES\n",
      "   JOHN SMITH --[works_at]--> TECHCORP INC.\n",
      "\n",
      "2. All 'located_in' relationships:\n",
      "   TECHCORP --[located_in]--> SAN FRANCISCO, CALIFORNIA\n",
      "   TECHCORP --[located_in]--> NEW YORK CITY, NEW YORK\n",
      "   TECHCORP --[located_in]--> NEW YORK, NEW YORK\n",
      "   TECHCORP INDUSTRIES --[located_in]--> SAN FRANCISCO BAY AREA\n",
      "\n",
      "3. SAME_AS relationships (duplicate links):\n",
      "   SAN FRANCISCO --[SAME_AS]--> SAN FRANCISCO, CALIFORNIA\n",
      "   TECHCORP 2025 ENGINEERING HIRING --[SAME_AS]--> TECHCORP 2025 ENGINEERING HIRING PLAN\n",
      "   TECHCORP NEW YORK EXPANSION (HIRING) --[SAME_AS]--> TECHCORP NEW YORK EXPANSION\n",
      "   TECHCORP EXPANSION TO NEW YORK CITY --[SAME_AS]--> TECHCORP NEW YORK EXPANSION\n",
      "   TECHCORP NEW YORK EXPANSION HIRING PLAN --[SAME_AS]--> TECHCORP NEW YORK EXPANSION\n",
      "   TECHCORP EXPANSION INTO NEW YORK CITY --[SAME_AS]--> TECHCORP NEW YORK EXPANSION\n",
      "   NEW YORK CITY --[SAME_AS]--> NEW YORK CITY, NEW YORK\n",
      "   NEW YORK, NEW YORK --[SAME_AS]--> NEW YORK CITY, NEW YORK\n",
      "   TECHCORP NEW YORK EXPANSION (HIRING) --[SAME_AS]--> TECHCORP EXPANSION INTO NEW YORK CITY\n",
      "   TECHCORP EXPANSION TO NEW YORK CITY --[SAME_AS]--> TECHCORP EXPANSION INTO NEW YORK CITY\n",
      "   TECHCORP NEW YORK EXPANSION HIRING PLAN --[SAME_AS]--> TECHCORP EXPANSION INTO NEW YORK CITY\n",
      "   TECHCORP NEW YORK EXPANSION --[SAME_AS]--> TECHCORP EXPANSION INTO NEW YORK CITY\n",
      "   NEW YORK CITY --[SAME_AS]--> NEW YORK, NEW YORK\n",
      "   NEW YORK CITY, NEW YORK --[SAME_AS]--> NEW YORK, NEW YORK\n",
      "   TECHCORP 2025 ENGINEERING HIRING PLAN --[SAME_AS]--> TECHCORP 2025 ENGINEERING HIRING\n",
      "   CTO, TECHCORP --[SAME_AS]--> CHIEF TECHNOLOGY OFFICER\n",
      "   CHIEF TECHNOLOGY OFFICER AT TECHCORP --[SAME_AS]--> CHIEF TECHNOLOGY OFFICER\n",
      "   NEW YORK, NEW YORK --[SAME_AS]--> NEW YORK CITY\n",
      "   NEW YORK CITY, NEW YORK --[SAME_AS]--> NEW YORK CITY\n",
      "   SAN FRANCISCO, CALIFORNIA --[SAME_AS]--> SAN FRANCISCO\n",
      "   TECHCORP NEW YORK EXPANSION (HIRING) --[SAME_AS]--> TECHCORP NEW YORK EXPANSION HIRING PLAN\n",
      "   TECHCORP EXPANSION TO NEW YORK CITY --[SAME_AS]--> TECHCORP NEW YORK EXPANSION HIRING PLAN\n",
      "   TECHCORP EXPANSION INTO NEW YORK CITY --[SAME_AS]--> TECHCORP NEW YORK EXPANSION HIRING PLAN\n",
      "   TECHCORP NEW YORK EXPANSION --[SAME_AS]--> TECHCORP NEW YORK EXPANSION HIRING PLAN\n",
      "   TECHCORP 2024 REVENUE --[SAME_AS]--> TECHCORP FY2024 REVENUE\n",
      "   CTO, TECHCORP --[SAME_AS]--> CHIEF TECHNOLOGY OFFICER AT TECHCORP\n",
      "   CHIEF TECHNOLOGY OFFICER --[SAME_AS]--> CHIEF TECHNOLOGY OFFICER AT TECHCORP\n",
      "   TECHCORP NEW YORK EXPANSION (HIRING) --[SAME_AS]--> TECHCORP EXPANSION TO NEW YORK CITY\n",
      "   TECHCORP NEW YORK EXPANSION HIRING PLAN --[SAME_AS]--> TECHCORP EXPANSION TO NEW YORK CITY\n",
      "   TECHCORP EXPANSION INTO NEW YORK CITY --[SAME_AS]--> TECHCORP EXPANSION TO NEW YORK CITY\n",
      "   TECHCORP NEW YORK EXPANSION --[SAME_AS]--> TECHCORP EXPANSION TO NEW YORK CITY\n",
      "   TECHCORP FY2024 REVENUE --[SAME_AS]--> TECHCORP 2024 REVENUE\n",
      "   CHIEF TECHNOLOGY OFFICER AT TECHCORP --[SAME_AS]--> CTO, TECHCORP\n",
      "   CHIEF TECHNOLOGY OFFICER --[SAME_AS]--> CTO, TECHCORP\n",
      "   TECHCORP EXPANSION TO NEW YORK CITY --[SAME_AS]--> TECHCORP NEW YORK EXPANSION (HIRING)\n",
      "   TECHCORP NEW YORK EXPANSION HIRING PLAN --[SAME_AS]--> TECHCORP NEW YORK EXPANSION (HIRING)\n",
      "   TECHCORP EXPANSION INTO NEW YORK CITY --[SAME_AS]--> TECHCORP NEW YORK EXPANSION (HIRING)\n",
      "   TECHCORP NEW YORK EXPANSION --[SAME_AS]--> TECHCORP NEW YORK EXPANSION (HIRING)\n"
     ]
    }
   ],
   "source": [
    "# Create persistent graph store\n",
    "graph_name = \"e2e_knowledge_graph\"\n",
    "kg_store = GraphStore(graph_name, config=config)\n",
    "kg_store.create_graph(graph_name)\n",
    "\n",
    "# Get all triples from temporary store (including SAME_AS edges)\n",
    "all_resolved_triples = temp_store.get_triples()\n",
    "\n",
    "# Insert into persistent graph\n",
    "print(\"Inserting resolved triples into knowledge graph...\")\n",
    "inserted = kg_store.add_triples(all_resolved_triples)\n",
    "print(f\"  Inserted {inserted} triples\")\n",
    "\n",
    "# Get graph statistics\n",
    "stats = kg_store.get_statistics()\n",
    "print(f\"\\n‚úÖ Knowledge graph created!\")\n",
    "print(f\"  Nodes: {stats['node_count']}\")\n",
    "print(f\"  Edges: {stats['edge_count']}\")\n",
    "print(f\"  Sources: {', '.join(stats['sources'])}\")\n",
    "print(f\"  Predicates: {', '.join(stats['predicates'][:10])}...\")\n",
    "\n",
    "# Query example relationships\n",
    "print(\"\\n\\nExample queries:\")\n",
    "print(\"\\n1. All 'works_at' relationships:\")\n",
    "works_at = kg_store.query_by_pattern(predicate=\"works_at\")\n",
    "for edge in works_at[:5]:\n",
    "    print(f\"   {edge['subject']} --[works_at]--> {edge['object']}\")\n",
    "\n",
    "print(\"\\n2. All 'located_in' relationships:\")\n",
    "located_in = kg_store.query_by_pattern(predicate=\"located_in\")\n",
    "for edge in located_in[:5]:\n",
    "    print(f\"   {edge['subject']} --[located_in]--> {edge['object']}\")\n",
    "\n",
    "print(\"\\n3. SAME_AS relationships (duplicate links):\")\n",
    "same_as = kg_store.query_by_pattern(predicate=\"SAME_AS\")\n",
    "for edge in same_as:\n",
    "    print(f\"   {edge['subject']} --[SAME_AS]--> {edge['object']}\")\n",
    "\n",
    "# Clean up temporary store\n",
    "temp_store.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analytics Visualization\n",
    "\n",
    "Query and display analytics from event logs showing LLM usage, costs, and performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics Store initialized\n",
      "  Database: c:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\logs\\analytics.db\n",
      "\n",
      "Fetching service events...\n",
      "  Total events: 100\n",
      "\n",
      "Events by service:\n",
      "  ingestion.service: 2 events\n",
      "  ingestion.pipeline: 90 events\n",
      "  ingestion.analytics: 3 events\n",
      "  ontology.recommender: 2 events\n",
      "  extractor: 3 events\n"
     ]
    }
   ],
   "source": [
    "# Initialize analytics store\n",
    "# Analytics database is typically in log_dir/analytics.db\n",
    "analytics_db_path = config.storage.log_dir / \"analytics.db\"\n",
    "analytics_store = AnalyticsStore(f\"sqlite:///{analytics_db_path}\", event_store=event_store)\n",
    "\n",
    "print(\"Analytics Store initialized\")\n",
    "print(f\"  Database: {analytics_db_path}\")\n",
    "\n",
    "# Fetch service events\n",
    "print(\"\\nFetching service events...\")\n",
    "all_events = analytics_store.fetch_service_events(limit=100)\n",
    "print(f\"  Total events: {len(all_events)}\")\n",
    "\n",
    "# Group by service\n",
    "by_service = {}\n",
    "for event in all_events:\n",
    "    service = event.service\n",
    "    if service not in by_service:\n",
    "        by_service[service] = []\n",
    "    by_service[service].append(event)\n",
    "\n",
    "print(f\"\\nEvents by service:\")\n",
    "for service, events in by_service.items():\n",
    "    print(f\"  {service}: {len(events)} events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ONTOLOGY RECOMMENDATION METRICS\n",
      "======================================================================\n",
      "Total calls: 1\n",
      "Total tokens: 6,216\n",
      "Total cost: $0.0000\n",
      "Avg latency: 60036.0 ms\n",
      "\n",
      "By scope:\n",
      "  balanced:\n",
      "    Calls: 1\n",
      "    Tokens: 6,216\n",
      "    Input tokens: 2,562\n",
      "    Output tokens: 3,654\n",
      "    Cost: $0.0000\n"
     ]
    }
   ],
   "source": [
    "# Display ontology recommendation metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ONTOLOGY RECOMMENDATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "ontology_metrics = ontology_recommendation_metrics(analytics_store)\n",
    "if ontology_metrics[\"total_calls\"] > 0:\n",
    "    print(f\"Total calls: {ontology_metrics['total_calls']}\")\n",
    "    print(f\"Total tokens: {ontology_metrics['total_tokens']:,}\")\n",
    "    if ontology_metrics.get('input_tokens'):\n",
    "        print(f\"  Input tokens: {ontology_metrics['input_tokens']:,}\")\n",
    "    if ontology_metrics.get('output_tokens'):\n",
    "        print(f\"  Output tokens: {ontology_metrics['output_tokens']:,}\")\n",
    "    print(f\"Total cost: ${ontology_metrics['total_cost']:.4f}\")\n",
    "    print(f\"Avg latency: {ontology_metrics['avg_latency_ms']:.1f} ms\")\n",
    "    \n",
    "    if ontology_metrics.get(\"by_scope\"):\n",
    "        print(\"\\nBy scope:\")\n",
    "        for scope, metrics in ontology_metrics[\"by_scope\"].items():\n",
    "            print(f\"  {scope}:\")\n",
    "            print(f\"    Calls: {metrics['calls']}\")\n",
    "            print(f\"    Tokens: {metrics['total_tokens']:,}\")\n",
    "            if metrics.get('input_tokens'):\n",
    "                print(f\"    Input tokens: {metrics['input_tokens']:,}\")\n",
    "            if metrics.get('output_tokens'):\n",
    "                print(f\"    Output tokens: {metrics['output_tokens']:,}\")\n",
    "            print(f\"    Cost: ${metrics['total_cost']:.4f}\")\n",
    "else:\n",
    "    print(\"No ontology recommendation metrics available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRIPLE EXTRACTION METRICS\n",
      "======================================================================\n",
      "Total calls: 3\n",
      "Total tokens: 18,961\n",
      "  Input tokens: 9,973\n",
      "  Output tokens: 8,988\n",
      "Total cost: $0.0000\n",
      "Avg latency: 43622.4 ms\n",
      "Total triples: 12\n",
      "\n",
      "By scope:\n",
      "  balanced:\n",
      "    Calls: 3\n",
      "    Tokens: 18,961\n",
      "    Input tokens: 9,973\n",
      "    Output tokens: 8,988\n",
      "    Cost: $0.0000\n",
      "    Triples: 12\n"
     ]
    }
   ],
   "source": [
    "# Display triple extraction metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRIPLE EXTRACTION METRICS\")\n",
    "print(\"=\"*70)\n",
    "extraction_metrics = triple_extraction_metrics(analytics_store)\n",
    "if extraction_metrics[\"total_calls\"] > 0:\n",
    "    print(f\"Total calls: {extraction_metrics['total_calls']}\")\n",
    "    print(f\"Total tokens: {extraction_metrics['total_tokens']:,}\")\n",
    "    # Aggregate input/output tokens from by_scope if available\n",
    "    total_input = sum(m.get('input_tokens', 0) or 0 for m in extraction_metrics.get('by_scope', {}).values())\n",
    "    total_output = sum(m.get('output_tokens', 0) or 0 for m in extraction_metrics.get('by_scope', {}).values())\n",
    "    if total_input > 0:\n",
    "        print(f\"  Input tokens: {total_input:,}\")\n",
    "    if total_output > 0:\n",
    "        print(f\"  Output tokens: {total_output:,}\")\n",
    "    print(f\"Total cost: ${extraction_metrics['total_cost']:.4f}\")\n",
    "    print(f\"Avg latency: {extraction_metrics['avg_latency_ms']:.1f} ms\")\n",
    "    print(f\"Total triples: {extraction_metrics['total_triples']}\")\n",
    "    \n",
    "    if extraction_metrics.get(\"by_scope\"):\n",
    "        print(\"\\nBy scope:\")\n",
    "        for scope, metrics in extraction_metrics[\"by_scope\"].items():\n",
    "            print(f\"  {scope}:\")\n",
    "            print(f\"    Calls: {metrics['calls']}\")\n",
    "            print(f\"    Tokens: {metrics['total_tokens']:,}\")\n",
    "            if metrics.get('input_tokens'):\n",
    "                print(f\"    Input tokens: {metrics['input_tokens']:,}\")\n",
    "            if metrics.get('output_tokens'):\n",
    "                print(f\"    Output tokens: {metrics['output_tokens']:,}\")\n",
    "            print(f\"    Cost: ${metrics['total_cost']:.4f}\")\n",
    "            print(f\"    Triples: {metrics['triples']}\")\n",
    "else:\n",
    "    print(\"No triple extraction metrics available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ENTITY RESOLUTION METRICS\n",
      "======================================================================\n",
      "Total calls: 23\n",
      "Total tokens: 36,779\n",
      "  Input tokens: 22,562\n",
      "  Output tokens: 14,217\n",
      "Total cost: $0.0000\n",
      "Avg latency: 0.0 ms\n",
      "\n",
      "By step:\n",
      "  Entity Matching:\n",
      "    Calls: 5\n",
      "    Tokens: 9,901\n",
      "    Input tokens: 5,849\n",
      "    Output tokens: 4,052\n",
      "    Cost: $0.0000\n",
      "  Edge Matching:\n",
      "    Calls: 18\n",
      "    Tokens: 26,878\n",
      "    Input tokens: 16,713\n",
      "    Output tokens: 10,165\n",
      "    Cost: $0.0000\n"
     ]
    }
   ],
   "source": [
    "# Display entity resolution metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTITY RESOLUTION METRICS\")\n",
    "print(\"=\"*70)\n",
    "resolution_metrics = entity_resolution_metrics(analytics_store)\n",
    "if resolution_metrics[\"total_calls\"] > 0:\n",
    "    print(f\"Total calls: {resolution_metrics['total_calls']}\")\n",
    "    print(f\"Total tokens: {resolution_metrics['total_tokens']:,}\")\n",
    "    # Aggregate input/output tokens from both steps\n",
    "    total_input = 0\n",
    "    total_output = 0\n",
    "    for step_name in [\"entity_matching\", \"edge_matching\"]:\n",
    "        step_metrics = resolution_metrics.get(step_name, {})\n",
    "        if step_metrics.get('input_tokens'):\n",
    "            total_input += step_metrics['input_tokens']\n",
    "        if step_metrics.get('output_tokens'):\n",
    "            total_output += step_metrics['output_tokens']\n",
    "    if total_input > 0:\n",
    "        print(f\"  Input tokens: {total_input:,}\")\n",
    "    if total_output > 0:\n",
    "        print(f\"  Output tokens: {total_output:,}\")\n",
    "    print(f\"Total cost: ${resolution_metrics['total_cost']:.4f}\")\n",
    "    print(f\"Avg latency: {resolution_metrics.get('avg_latency_ms', 0):.1f} ms\")\n",
    "    \n",
    "    print(\"\\nBy step:\")\n",
    "    print(\"  Entity Matching:\")\n",
    "    em = resolution_metrics[\"entity_matching\"]\n",
    "    print(f\"    Calls: {em['calls']}\")\n",
    "    print(f\"    Tokens: {em['total_tokens']:,}\")\n",
    "    if em.get('input_tokens'):\n",
    "        print(f\"    Input tokens: {em['input_tokens']:,}\")\n",
    "    if em.get('output_tokens'):\n",
    "        print(f\"    Output tokens: {em['output_tokens']:,}\")\n",
    "    print(f\"    Cost: ${em['total_cost']:.4f}\")\n",
    "    \n",
    "    print(\"  Edge Matching:\")\n",
    "    ed = resolution_metrics[\"edge_matching\"]\n",
    "    print(f\"    Calls: {ed['calls']}\")\n",
    "    print(f\"    Tokens: {ed['total_tokens']:,}\")\n",
    "    if ed.get('input_tokens'):\n",
    "        print(f\"    Input tokens: {ed['input_tokens']:,}\")\n",
    "    if ed.get('output_tokens'):\n",
    "        print(f\"    Output tokens: {ed['output_tokens']:,}\")\n",
    "    print(f\"    Cost: ${ed['total_cost']:.4f}\")\n",
    "else:\n",
    "    print(\"No entity resolution metrics available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CORPUS OVERVIEW\n",
      "======================================================================\n",
      "Total documents: 3\n",
      "Total tokens: 417\n",
      "Avg tokens/doc: 139.0\n",
      "Avg chunks/doc: 2.0\n",
      "\n",
      "Context strategy distribution:\n",
      "  document: 3\n"
     ]
    }
   ],
   "source": [
    "# Display corpus overview\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORPUS OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "overview = corpus_overview(analytics_store)\n",
    "print(f\"Total documents: {overview['documents']}\")\n",
    "print(f\"Total tokens: {overview['total_tokens']:,}\")\n",
    "print(f\"Avg tokens/doc: {overview['avg_tokens']:.1f}\")\n",
    "print(f\"Avg chunks/doc: {overview['avg_chunks']:.1f}\")\n",
    "\n",
    "if overview.get(\"context_strategy_counts\"):\n",
    "    print(\"\\nContext strategy distribution:\")\n",
    "    for strategy, count in overview[\"context_strategy_counts\"].items():\n",
    "        print(f\"  {strategy}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Launch Dashboard\n",
    "\n",
    "You can also launch the interactive Streamlit dashboard to explore analytics visually:\n",
    "\n",
    "```python\n",
    "from spindle.dashboard.app import cli_main\n",
    "import sys\n",
    "\n",
    "# Launch dashboard\n",
    "cli_main([\"--database\", str(analytics_db_path)])\n",
    "```\n",
    "\n",
    "Or run from command line:\n",
    "```bash\n",
    "python -m spindle.dashboard.app --database path/to/analytics.db\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete Spindle workflow:\n",
    "\n",
    "‚úÖ **Configuration**: Set up SpindleConfig in `example_project/` directory  \n",
    "‚úÖ **Document Creation**: Created 3 documents with entity variations  \n",
    "‚úÖ **Ingestion**: Ingested documents into document storage and vector storage  \n",
    "‚úÖ **Ontology Recommendation**: Iteratively recommended and extended ontology across documents  \n",
    "‚úÖ **Triple Extraction**: Extracted knowledge graph triples using the final ontology  \n",
    "‚úÖ **Entity Resolution**: Identified and linked duplicate entities (TechCorp variants, John Smith variants)  \n",
    "‚úÖ **Knowledge Graph**: Inserted resolved triples into persistent graph  \n",
    "‚úÖ **Analytics**: Viewed metrics from event logs showing LLM usage and costs  \n",
    "\n",
    "### Key Results\n",
    "\n",
    "- **Documents**: 3 documents ingested\n",
    "- **Triples**: Extracted triples with entity consistency\n",
    "- **Entity Resolution**: Found duplicate clusters for company and person name variations\n",
    "- **Knowledge Graph**: Persistent graph with resolved entities and relationships\n",
    "- **Analytics**: Complete observability into LLM operations and costs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Query the knowledge graph using Cypher queries\n",
    "- Extend the ontology for new document types\n",
    "- Fine-tune entity resolution thresholds\n",
    "- Explore the dashboard for detailed analytics\n",
    "- Export triples for use in other systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kg_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cleanup: Close stores\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mkg_store\u001b[49m.close()\n\u001b[32m      3\u001b[39m vector_store.close()\n\u001b[32m      4\u001b[39m detach_observer()  \u001b[38;5;66;03m# Remove persistent observer\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'kg_store' is not defined"
     ]
    }
   ],
   "source": [
    "# Cleanup: Close stores\n",
    "kg_store.close()\n",
    "vector_store.close()\n",
    "detach_observer()  # Remove persistent observer\n",
    "print(\"‚úÖ Stores closed and event observer detached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detach_observer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
