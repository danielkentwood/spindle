{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analytics Database Queries\n",
        "\n",
        "This notebook demonstrates how to query the Spindle analytics SQLite database using both direct SQL queries and the AnalyticsStore API.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The analytics database stores:\n",
        "- **Document Observations**: Structured metrics about ingested documents\n",
        "- **Service Events**: LLM calls, latency, costs, and other observability data\n",
        "\n",
        "## Database Schema\n",
        "\n",
        "- `ingestion_observations`: Main table storing document observations as JSON\n",
        "- `ingestion_observation_events`: Service events linked to observations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Imports successful\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Spindle imports\n",
        "from spindle.analytics import AnalyticsStore\n",
        "from spindle.analytics.views import (\n",
        "    corpus_overview,\n",
        "    document_size_table,\n",
        "    chunk_window_risk,\n",
        "    observability_events,\n",
        "    ontology_recommendation_metrics,\n",
        "    triple_extraction_metrics,\n",
        "    entity_resolution_metrics,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Connect to the Analytics Database\n",
        "\n",
        "First, we'll locate and connect to the analytics database. By default, it's stored at `{project_root}/logs/analytics.db`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found database at: example_project\\logs\\analytics.db\n",
            "   Database size: 76.00 KB\n",
            "‚úÖ Connected to database: sqlite:///C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\logs\\analytics.db\n"
          ]
        }
      ],
      "source": [
        "# Path to analytics database\n",
        "# Update this path to point to your analytics database\n",
        "ANALYTICS_DB_PATH = Path(\"example_project/logs/analytics.db\")\n",
        "\n",
        "# Check if database exists\n",
        "if not ANALYTICS_DB_PATH.exists():\n",
        "    print(f\"‚ö†Ô∏è  Database not found at: {ANALYTICS_DB_PATH}\")\n",
        "    print(\"   Please update ANALYTICS_DB_PATH to point to your analytics database\")\n",
        "    print(\"   Or run the example_e2e.ipynb notebook first to generate sample data\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found database at: {ANALYTICS_DB_PATH}\")\n",
        "    print(f\"   Database size: {ANALYTICS_DB_PATH.stat().st_size / 1024:.2f} KB\")\n",
        "\n",
        "# Create AnalyticsStore instance\n",
        "database_url = f\"sqlite:///{ANALYTICS_DB_PATH.resolve()}\"\n",
        "store = AnalyticsStore(database_url)\n",
        "print(f\"‚úÖ Connected to database: {database_url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Explore Database Schema\n",
        "\n",
        "Let's examine the database structure using direct SQL queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "events = store.fetch_service_events(service=\"ontology.recommender\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'timestamp': datetime.datetime(2025, 11, 16, 8, 0, 48, 839496),\n",
              " 'service': 'ontology.recommender',\n",
              " 'name': 'recommend.start',\n",
              " 'payload': {'scope': 'balanced', 'text_length': 1788}}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "events[0].__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'timestamp': datetime.datetime(2025, 11, 16, 8, 1, 50, 745960),\n",
              " 'service': 'ontology.recommender',\n",
              " 'name': 'recommend.complete',\n",
              " 'payload': {'scope': 'balanced',\n",
              "  'entity_type_count': 7,\n",
              "  'relation_type_count': 15,\n",
              "  'input_tokens': 2562,\n",
              "  'output_tokens': 4276,\n",
              "  'cost': 0.009193,\n",
              "  'latency_ms': 61119.81400009245,\n",
              "  'model': 'gpt-5-mini-2025-08-07'}}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "events[1].__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 655078), service='ingestion.service', name='run.start', payload={'path_count': 3, 'catalog_url': 'sqlite:///c:\\\\Users\\\\danie\\\\Repos\\\\spindle\\\\spindle\\\\notebooks\\\\example_project\\\\catalog\\\\ingestion.db', 'vector_store_uri': 'c:\\\\Users\\\\danie\\\\Repos\\\\spindle\\\\spindle\\\\notebooks\\\\example_project\\\\vector_store'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 677214), service='ingestion.pipeline', name='stage_start', payload={'stage': 'checksum'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 677214), service='ingestion.pipeline', name='stage_start', payload={'stage': 'checksum'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 680789), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'checksum', 'duration_ms': 4.391300026327372}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 680789), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'checksum', 'duration_ms': 4.391300026327372}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 684381), service='ingestion.pipeline', name='stage_start', payload={'stage': 'load'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 684381), service='ingestion.pipeline', name='stage_start', payload={'stage': 'load'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 702765), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'load', 'duration_ms': 18.162999767810106}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 702765), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'load', 'duration_ms': 18.162999767810106}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 705825), service='ingestion.pipeline', name='stage_start', payload={'stage': 'preprocess'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 705825), service='ingestion.pipeline', name='stage_start', payload={'stage': 'preprocess'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 707529), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'preprocess', 'duration_ms': 3.297900315374136}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 707529), service='ingestion.pipeline', name='stage_start', payload={'stage': 'split'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 707529), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'split', 'duration_ms': 2.8897998854517937}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 707529), service='ingestion.pipeline', name='stage_start', payload={'stage': 'metadata'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 707529), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'metadata', 'duration_ms': 2.404300030320883}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 707529), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'preprocess', 'duration_ms': 3.297900315374136}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 707529), service='ingestion.pipeline', name='stage_start', payload={'stage': 'split'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 707529), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'split', 'duration_ms': 2.8897998854517937}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 707529), service='ingestion.pipeline', name='stage_start', payload={'stage': 'metadata'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 707529), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'metadata', 'duration_ms': 2.404300030320883}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 723441), service='ingestion.pipeline', name='stage_start', payload={'stage': 'chunks'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 723441), service='ingestion.pipeline', name='stage_start', payload={'stage': 'chunks'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 726526), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'chunks', 'duration_ms': 2.7314997278153896}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 726526), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'chunks', 'duration_ms': 2.7314997278153896}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 729133), service='ingestion.pipeline', name='stage_start', payload={'stage': 'graph'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 729133), service='ingestion.pipeline', name='stage_start', payload={'stage': 'graph'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 731924), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'graph', 'duration_ms': 2.6051001623272896}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 731924), service='ingestion.pipeline', name='graph_built', payload={'document_id': '612fe132f4354e15a7cf53201370ff50', 'chunk_count': 4}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 731924), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'graph', 'duration_ms': 2.6051001623272896}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 731924), service='ingestion.pipeline', name='graph_built', payload={'document_id': '612fe132f4354e15a7cf53201370ff50', 'chunk_count': 4}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 773036), service='ingestion.pipeline', name='stage_start', payload={'stage': 'checksum'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 773036), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'checksum', 'duration_ms': 3.665999975055456}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 773036), service='ingestion.pipeline', name='stage_start', payload={'stage': 'load'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 773036), service='ingestion.pipeline', name='stage_start', payload={'stage': 'checksum'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 773036), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'checksum', 'duration_ms': 3.665999975055456}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 773036), service='ingestion.pipeline', name='stage_start', payload={'stage': 'load'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 789798), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'load', 'duration_ms': 2.7598999440670013}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 789798), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'load', 'duration_ms': 2.7598999440670013}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 792650), service='ingestion.pipeline', name='stage_start', payload={'stage': 'preprocess'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 792650), service='ingestion.pipeline', name='stage_start', payload={'stage': 'preprocess'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 795661), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'preprocess', 'duration_ms': 2.6237000711262226}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 795661), service='ingestion.pipeline', name='stage_start', payload={'stage': 'split'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 795661), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'split', 'duration_ms': 2.685100305825472}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 795661), service='ingestion.pipeline', name='stage_start', payload={'stage': 'metadata'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 795661), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'preprocess', 'duration_ms': 2.6237000711262226}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 795661), service='ingestion.pipeline', name='stage_start', payload={'stage': 'split'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 795661), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'split', 'duration_ms': 2.685100305825472}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 795661), service='ingestion.pipeline', name='stage_start', payload={'stage': 'metadata'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 806908), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'metadata', 'duration_ms': 3.3058002591133118}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 806908), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'metadata', 'duration_ms': 3.3058002591133118}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 807959), service='ingestion.pipeline', name='stage_start', payload={'stage': 'chunks'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 807959), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'chunks', 'duration_ms': 2.8663002885878086}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 807959), service='ingestion.pipeline', name='stage_start', payload={'stage': 'graph'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 807959), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'graph', 'duration_ms': 2.872999757528305}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 807959), service='ingestion.pipeline', name='stage_start', payload={'stage': 'chunks'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 807959), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'chunks', 'duration_ms': 2.8663002885878086}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 807959), service='ingestion.pipeline', name='stage_start', payload={'stage': 'graph'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 807959), service='ingestion.pipeline', name='graph_built', payload={'document_id': '81aada0340354d91b20ea8cadf4af35f', 'chunk_count': 1}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 807959), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'graph', 'duration_ms': 2.872999757528305}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 807959), service='ingestion.pipeline', name='graph_built', payload={'document_id': '81aada0340354d91b20ea8cadf4af35f', 'chunk_count': 1}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 873790), service='ingestion.pipeline', name='stage_start', payload={'stage': 'checksum'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 873790), service='ingestion.pipeline', name='stage_start', payload={'stage': 'checksum'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 879542), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'checksum', 'duration_ms': 9.66209964826703}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 879542), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'checksum', 'duration_ms': 9.66209964826703}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 888642), service='ingestion.pipeline', name='stage_start', payload={'stage': 'load'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 888642), service='ingestion.pipeline', name='stage_start', payload={'stage': 'load'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 892331), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'load', 'duration_ms': 3.727399744093418}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 892331), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'load', 'duration_ms': 3.727399744093418}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 895787), service='ingestion.pipeline', name='stage_start', payload={'stage': 'preprocess'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 895787), service='ingestion.pipeline', name='stage_start', payload={'stage': 'preprocess'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 898919), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'preprocess', 'duration_ms': 2.8602001257240772}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 898919), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'preprocess', 'duration_ms': 2.8602001257240772}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 902066), service='ingestion.pipeline', name='stage_start', payload={'stage': 'split'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 902066), service='ingestion.pipeline', name='stage_start', payload={'stage': 'split'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 906450), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'split', 'duration_ms': 4.178300034254789}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 906450), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'split', 'duration_ms': 4.178300034254789}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 908155), service='ingestion.pipeline', name='stage_start', payload={'stage': 'metadata'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 908155), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'metadata', 'duration_ms': 4.226500168442726}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 908155), service='ingestion.pipeline', name='stage_start', payload={'stage': 'chunks'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 908155), service='ingestion.pipeline', name='stage_start', payload={'stage': 'metadata'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 908155), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'metadata', 'duration_ms': 4.226500168442726}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 908155), service='ingestion.pipeline', name='stage_start', payload={'stage': 'chunks'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 923225), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'chunks', 'duration_ms': 4.161300137639046}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 923225), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'chunks', 'duration_ms': 4.161300137639046}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 924773), service='ingestion.pipeline', name='stage_start', payload={'stage': 'graph'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 924773), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'graph', 'duration_ms': 3.9798999205231667}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 924773), service='ingestion.pipeline', name='stage_start', payload={'stage': 'graph'}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 924773), service='ingestion.pipeline', name='graph_built', payload={'document_id': '398a88c792f5415f9b117e3a59d909b7', 'chunk_count': 1}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 924773), service='ingestion.pipeline', name='stage_complete', payload={'stage': 'graph', 'duration_ms': 3.9798999205231667}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 44, 924773), service='ingestion.pipeline', name='graph_built', payload={'document_id': '398a88c792f5415f9b117e3a59d909b7', 'chunk_count': 1}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 45, 340977), service='ingestion.analytics', name='document.observed', payload={'document_id': '612fe132f4354e15a7cf53201370ff50', 'observation': {'schema_version': '1.0.0', 'metadata': {'document_id': '612fe132f4354e15a7cf53201370ff50', 'source_uri': 'C:\\\\Users\\\\danie\\\\Repos\\\\spindle\\\\spindle\\\\notebooks\\\\example_project\\\\documents\\\\doc1_techcorp_overview.md', 'source_type': 'file', 'content_type': None, 'language': None, 'ingested_at': '2025-11-16T07:11:44.726526', 'hash_signature': '1ce06bc4b5f3b8ca8258467908458c9ec2383528e8f2d1856f376b9dc10fb547'}, 'structural': {'token_count': 242, 'character_count': 1782, 'page_count': None, 'section_count': None, 'average_tokens_per_section': None, 'chunk_count': 4, 'chunk_token_summary': {'minimum': 47.0, 'maximum': 67.0, 'median': 64.0, 'mean': 60.5, 'p95': 66.7}}, 'chunk_windows': [{'window_size': 2, 'token_summary': {'minimum': 110.0, 'maximum': 132.0, 'median': 130.0, 'mean': 124.0, 'p95': 131.8}, 'overlap_tokens': None, 'overlap_ratio': None, 'cross_chunk_link_rate': 0.0, 'context_limit_risk': 'low'}, {'window_size': 3, 'token_summary': {'minimum': 177.0, 'maximum': 195.0, 'median': 186.0, 'mean': 186.0, 'p95': 194.1}, 'overlap_tokens': None, 'overlap_ratio': None, 'cross_chunk_link_rate': 0.0, 'context_limit_risk': 'low'}], 'segments': {'segment_boundaries': [], 'segment_token_summary': None, 'embedding_dispersion': None, 'topic_transition_score': None}, 'ontology': None, 'context': {'recommended_strategy': 'document', 'supporting_risk': 'low', 'estimated_token_usage': 242, 'target_token_budget': 12000}, 'observability': {'service_events': [{'timestamp': '2025-11-16T07:11:44.731924', 'service': 'ingestion.pipeline', 'name': 'graph_built', 'payload': {'document_id': '612fe132f4354e15a7cf53201370ff50', 'chunk_count': 4}}], 'error_signals': [], 'latency_breakdown': {}, 'has_errors': False}}}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 45, 340977), service='ingestion.analytics', name='document.observed', payload={'document_id': '81aada0340354d91b20ea8cadf4af35f', 'observation': {'schema_version': '1.0.0', 'metadata': {'document_id': '81aada0340354d91b20ea8cadf4af35f', 'source_uri': 'C:\\\\Users\\\\danie\\\\Repos\\\\spindle\\\\spindle\\\\notebooks\\\\example_project\\\\documents\\\\doc2_techcorp_expansion.md', 'source_type': 'file', 'content_type': None, 'language': None, 'ingested_at': '2025-11-16T07:11:44.807959', 'hash_signature': 'af0269bfe4397df84167acadf9b49a81f848963a024d71631634ad6d4a064c16'}, 'structural': {'token_count': 92, 'character_count': 553, 'page_count': None, 'section_count': None, 'average_tokens_per_section': None, 'chunk_count': 1, 'chunk_token_summary': {'minimum': 92.0, 'maximum': 92.0, 'median': 92.0, 'mean': 92.0, 'p95': 92.0}}, 'chunk_windows': [], 'segments': {'segment_boundaries': [], 'segment_token_summary': None, 'embedding_dispersion': None, 'topic_transition_score': None}, 'ontology': None, 'context': {'recommended_strategy': 'document', 'supporting_risk': 'low', 'estimated_token_usage': 92, 'target_token_budget': 12000}, 'observability': {'service_events': [{'timestamp': '2025-11-16T07:11:44.807959', 'service': 'ingestion.pipeline', 'name': 'graph_built', 'payload': {'document_id': '81aada0340354d91b20ea8cadf4af35f', 'chunk_count': 1}}], 'error_signals': [], 'latency_breakdown': {}, 'has_errors': False}}}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 45, 340977), service='ingestion.analytics', name='document.observed', payload={'document_id': '398a88c792f5415f9b117e3a59d909b7', 'observation': {'schema_version': '1.0.0', 'metadata': {'document_id': '398a88c792f5415f9b117e3a59d909b7', 'source_uri': 'C:\\\\Users\\\\danie\\\\Repos\\\\spindle\\\\spindle\\\\notebooks\\\\example_project\\\\documents\\\\doc3_techcorp_financials.md', 'source_type': 'file', 'content_type': None, 'language': None, 'ingested_at': '2025-11-16T07:11:44.923225', 'hash_signature': '3749f76798488cec6610bad87fcb22a8fe6e8137728aa3d4ffa204d9e4d9da68'}, 'structural': {'token_count': 83, 'character_count': 583, 'page_count': None, 'section_count': None, 'average_tokens_per_section': None, 'chunk_count': 1, 'chunk_token_summary': {'minimum': 83.0, 'maximum': 83.0, 'median': 83.0, 'mean': 83.0, 'p95': 83.0}}, 'chunk_windows': [], 'segments': {'segment_boundaries': [], 'segment_token_summary': None, 'embedding_dispersion': None, 'topic_transition_score': None}, 'ontology': None, 'context': {'recommended_strategy': 'document', 'supporting_risk': 'low', 'estimated_token_usage': 83, 'target_token_budget': 12000}, 'observability': {'service_events': [{'timestamp': '2025-11-16T07:11:44.924773', 'service': 'ingestion.pipeline', 'name': 'graph_built', 'payload': {'document_id': '398a88c792f5415f9b117e3a59d909b7', 'chunk_count': 1}}], 'error_signals': [], 'latency_breakdown': {}, 'has_errors': False}}}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 45, 388428), service='ingestion.service', name='run.complete', payload={'path_count': 3, 'processed_documents': 3, 'processed_chunks': 6, 'errors': []}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 11, 45, 394558), service='ontology.recommender', name='recommend.start', payload={'scope': 'balanced', 'text_length': 1788}),\n",
              " ServiceEventRecord(timestamp=datetime.datetime(2025, 11, 16, 7, 12, 43, 173090), service='ontology.recommender', name='recommend.complete', payload={'scope': 'balanced', 'entity_type_count': 9, 'relation_type_count': 14, 'input_tokens': 2562, 'output_tokens': 4194, 'cost': 0.0, 'latency_ms': 56755.84980007261, 'model': 'CustomFast'})]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "store.fetch_service_events()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "store.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Tables in database:\n",
            "                           name\n",
            "0                service_events\n",
            "1        ingestion_observations\n",
            "2  ingestion_observation_events\n",
            "\n",
            "üìã Schema for 'service_events':\n",
            " cid      name     type  notnull dflt_value  pk\n",
            "   0        id  INTEGER        1       None   1\n",
            "   1 timestamp DATETIME        1       None   0\n",
            "   2   service  VARCHAR        1       None   0\n",
            "   3      name  VARCHAR        1       None   0\n",
            "   4   payload     JSON        1       None   0\n",
            "\n",
            "üìã Schema for 'ingestion_observations':\n",
            " cid           name     type  notnull dflt_value  pk\n",
            "   0             id  INTEGER        1       None   1\n",
            "   1    document_id  VARCHAR        1       None   0\n",
            "   2    ingested_at DATETIME        1       None   0\n",
            "   3 schema_version  VARCHAR        1       None   0\n",
            "   4        payload     JSON        1       None   0\n",
            "   5     created_at DATETIME        1       None   0\n",
            "\n",
            "üìã Schema for 'ingestion_observation_events':\n",
            " cid           name     type  notnull dflt_value  pk\n",
            "   0             id  INTEGER        1       None   1\n",
            "   1 observation_id  INTEGER        1       None   0\n",
            "   2      timestamp DATETIME        1       None   0\n",
            "   3        service  VARCHAR        1       None   0\n",
            "   4           name  VARCHAR        1       None   0\n",
            "   5        payload     JSON        1       None   0\n"
          ]
        }
      ],
      "source": [
        "# Connect directly to SQLite for schema exploration\n",
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# List all tables\n",
        "tables_query = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
        "tables = pd.read_sql_query(tables_query, conn)\n",
        "print(\"üìä Tables in database:\")\n",
        "print(tables)\n",
        "\n",
        "# Get schema for each table\n",
        "for table_name in tables['name']:\n",
        "    print(f\"\\nüìã Schema for '{table_name}':\")\n",
        "    schema_query = f\"PRAGMA table_info({table_name})\"\n",
        "    schema = pd.read_sql_query(schema_query, conn)\n",
        "    print(schema.to_string(index=False))\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Basic Queries Using AnalyticsStore API\n",
        "\n",
        "The AnalyticsStore provides convenient methods for querying observations and events.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Found 3 observations (showing up to 10)\n",
            "\n",
            "üìã Sample observation:\n",
            "  Document ID: eac54e99f2a94080964eafd9c6af1902\n",
            "  Ingested at: 2025-11-14 22:18:11.425474\n",
            "  Token count: 83\n",
            "  Chunk count: 1\n",
            "  Source URI: C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\documents\\doc3_techcorp_financials.md\n",
            "  Schema version: 1.0.0\n"
          ]
        }
      ],
      "source": [
        "# Fetch all observations (limit to 10 for display)\n",
        "observations = store.fetch_observations(limit=10)\n",
        "print(f\"üìÑ Found {len(observations)} observations (showing up to 10)\")\n",
        "\n",
        "if observations:\n",
        "    print(\"\\nüìã Sample observation:\")\n",
        "    obs = observations[0]\n",
        "    print(f\"  Document ID: {obs.metadata.document_id}\")\n",
        "    print(f\"  Ingested at: {obs.metadata.ingested_at}\")\n",
        "    print(f\"  Token count: {obs.structural.token_count}\")\n",
        "    print(f\"  Chunk count: {obs.structural.chunk_count}\")\n",
        "    print(f\"  Source URI: {obs.metadata.source_uri}\")\n",
        "    print(f\"  Schema version: {obs.schema_version}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No observations found in database\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîî Found 20 service events (showing up to 20)\n",
            "\n",
            "üìã Sample events:\n",
            "\n",
            "  Event 1:\n",
            "    Service: ingestion.service\n",
            "    Name: run.start\n",
            "    Timestamp: 2025-11-14 22:18:11.157642\n",
            "    Payload keys: ['path_count', 'catalog_url', 'vector_store_uri']\n",
            "\n",
            "  Event 2:\n",
            "    Service: ingestion.pipeline\n",
            "    Name: stage_start\n",
            "    Timestamp: 2025-11-14 22:18:11.170897\n",
            "    Payload keys: ['stage']\n",
            "\n",
            "  Event 3:\n",
            "    Service: ingestion.pipeline\n",
            "    Name: stage_start\n",
            "    Timestamp: 2025-11-14 22:18:11.170897\n",
            "    Payload keys: ['stage']\n",
            "\n",
            "  Event 4:\n",
            "    Service: ingestion.pipeline\n",
            "    Name: stage_complete\n",
            "    Timestamp: 2025-11-14 22:18:11.184624\n",
            "    Payload keys: ['stage', 'duration_ms']\n",
            "\n",
            "  Event 5:\n",
            "    Service: ingestion.pipeline\n",
            "    Name: stage_complete\n",
            "    Timestamp: 2025-11-14 22:18:11.184624\n",
            "    Payload keys: ['stage', 'duration_ms']\n"
          ]
        }
      ],
      "source": [
        "# Fetch service events\n",
        "events = store.fetch_service_events(limit=20)\n",
        "print(f\"üîî Found {len(events)} service events (showing up to 20)\")\n",
        "\n",
        "if events:\n",
        "    print(\"\\nüìã Sample events:\")\n",
        "    for i, event in enumerate(events[:5], 1):\n",
        "        print(f\"\\n  Event {i}:\")\n",
        "        print(f\"    Service: {event.service}\")\n",
        "        print(f\"    Name: {event.name}\")\n",
        "        print(f\"    Timestamp: {event.timestamp}\")\n",
        "        print(f\"    Payload keys: {list(event.payload.keys())}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No service events found in database\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Direct SQL Queries\n",
        "\n",
        "Let's perform some direct SQL queries for more detailed analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Query 1: Total observations\n",
            " total_observations\n",
            "                  3\n",
            "\n",
            "üìä Query 2: Recent document observations\n",
            "                     document_id                ingested_at schema_version                 created_at\n",
            "eac54e99f2a94080964eafd9c6af1902 2025-11-14 22:18:11.425474          1.0.0 2025-11-14 22:18:11.919544\n",
            "2a6b68c8b120469ea6cb21707ecbfeaf 2025-11-14 22:18:11.327198          1.0.0 2025-11-14 22:18:11.919544\n",
            "3f857ca539ef4595ae6b2775c23f07ed 2025-11-14 22:18:11.233879          1.0.0 2025-11-14 22:18:11.914128\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# Query 1: Count total observations\n",
        "query1 = \"SELECT COUNT(*) as total_observations FROM ingestion_observations\"\n",
        "result1 = pd.read_sql_query(query1, conn)\n",
        "print(\"üìä Query 1: Total observations\")\n",
        "print(result1.to_string(index=False))\n",
        "\n",
        "# Query 2: List all document IDs with ingestion timestamps\n",
        "query2 = \"\"\"\n",
        "SELECT \n",
        "    document_id,\n",
        "    ingested_at,\n",
        "    schema_version,\n",
        "    created_at\n",
        "FROM ingestion_observations\n",
        "ORDER BY ingested_at DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "result2 = pd.read_sql_query(query2, conn)\n",
        "print(\"\\nüìä Query 2: Recent document observations\")\n",
        "print(result2.to_string(index=False))\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Query 3: Service events by service\n",
            "           service  event_count\n",
            "ingestion.pipeline            3\n",
            "\n",
            "üìä Query 4: Top event types\n",
            "       name  event_count\n",
            "graph_built            3\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# Query 3: Count service events by service name\n",
        "query3 = \"\"\"\n",
        "SELECT \n",
        "    service,\n",
        "    COUNT(*) as event_count\n",
        "FROM ingestion_observation_events\n",
        "GROUP BY service\n",
        "ORDER BY event_count DESC\n",
        "\"\"\"\n",
        "result3 = pd.read_sql_query(query3, conn)\n",
        "print(\"üìä Query 3: Service events by service\")\n",
        "print(result3.to_string(index=False))\n",
        "\n",
        "# Query 4: Count service events by event name\n",
        "query4 = \"\"\"\n",
        "SELECT \n",
        "    name,\n",
        "    COUNT(*) as event_count\n",
        "FROM ingestion_observation_events\n",
        "GROUP BY name\n",
        "ORDER BY event_count DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "result4 = pd.read_sql_query(query4, conn)\n",
        "print(\"\\nüìä Query 4: Top event types\")\n",
        "print(result4.to_string(index=False))\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Query 5: Observations with event counts\n",
            "                     document_id                ingested_at  event_count  unique_services\n",
            "3f857ca539ef4595ae6b2775c23f07ed 2025-11-14 22:18:11.233879            1                1\n",
            "2a6b68c8b120469ea6cb21707ecbfeaf 2025-11-14 22:18:11.327198            1                1\n",
            "eac54e99f2a94080964eafd9c6af1902 2025-11-14 22:18:11.425474            1                1\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# Query 5: Join observations with their events\n",
        "query5 = \"\"\"\n",
        "SELECT \n",
        "    o.document_id,\n",
        "    o.ingested_at,\n",
        "    COUNT(e.id) as event_count,\n",
        "    COUNT(DISTINCT e.service) as unique_services\n",
        "FROM ingestion_observations o\n",
        "LEFT JOIN ingestion_observation_events e ON o.id = e.observation_id\n",
        "GROUP BY o.id, o.document_id, o.ingested_at\n",
        "ORDER BY event_count DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "result5 = pd.read_sql_query(query5, conn)\n",
        "print(\"üìä Query 5: Observations with event counts\")\n",
        "print(result5.to_string(index=False))\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Extract JSON Data from Observations\n",
        "\n",
        "The observation payloads are stored as JSON. Let's extract specific fields for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Query 6: Document structural metrics\n",
            "                     document_id                ingested_at  token_count  chunk_count                                                                                           source_uri source_type\n",
            "3f857ca539ef4595ae6b2775c23f07ed 2025-11-14 22:18:11.233879          242            4   C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\documents\\doc1_techcorp_overview.md        file\n",
            "2a6b68c8b120469ea6cb21707ecbfeaf 2025-11-14 22:18:11.327198           92            1  C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\documents\\doc2_techcorp_expansion.md        file\n",
            "eac54e99f2a94080964eafd9c6af1902 2025-11-14 22:18:11.425474           83            1 C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\documents\\doc3_techcorp_financials.md        file\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# Query 6: Extract token counts from JSON payload\n",
        "query6 = \"\"\"\n",
        "SELECT \n",
        "    document_id,\n",
        "    ingested_at,\n",
        "    json_extract(payload, '$.structural.token_count') as token_count,\n",
        "    json_extract(payload, '$.structural.chunk_count') as chunk_count,\n",
        "    json_extract(payload, '$.metadata.source_uri') as source_uri,\n",
        "    json_extract(payload, '$.metadata.source_type') as source_type\n",
        "FROM ingestion_observations\n",
        "ORDER BY token_count DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "result6 = pd.read_sql_query(query6, conn)\n",
        "print(\"üìä Query 6: Document structural metrics\")\n",
        "print(result6.to_string(index=False))\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Query 7: Context window assessments\n",
            "                     document_id recommended_strategy risk_level  estimated_tokens\n",
            "3f857ca539ef4595ae6b2775c23f07ed             document        low               242\n",
            "2a6b68c8b120469ea6cb21707ecbfeaf             document        low                92\n",
            "eac54e99f2a94080964eafd9c6af1902             document        low                83\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# Query 7: Extract context strategy recommendations\n",
        "query7 = \"\"\"\n",
        "SELECT \n",
        "    document_id,\n",
        "    json_extract(payload, '$.context.recommended_strategy') as recommended_strategy,\n",
        "    json_extract(payload, '$.context.supporting_risk') as risk_level,\n",
        "    json_extract(payload, '$.context.estimated_token_usage') as estimated_tokens\n",
        "FROM ingestion_observations\n",
        "WHERE json_extract(payload, '$.context') IS NOT NULL\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "result7 = pd.read_sql_query(query7, conn)\n",
        "print(\"üìä Query 7: Context window assessments\")\n",
        "if not result7.empty:\n",
        "    print(result7.to_string(index=False))\n",
        "else:\n",
        "    print(\"No context assessments found\")\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Analyze Service Event Payloads\n",
        "\n",
        "Service events contain detailed LLM metrics. Let's extract cost and latency data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Query 8: LLM usage metrics from events\n",
            "No LLM metrics found in events\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# Query 8: Extract LLM cost and token usage from event payloads\n",
        "query8 = \"\"\"\n",
        "SELECT \n",
        "    service,\n",
        "    name,\n",
        "    timestamp,\n",
        "    json_extract(payload, '$.model') as model,\n",
        "    json_extract(payload, '$.total_tokens') as total_tokens,\n",
        "    json_extract(payload, '$.input_tokens') as input_tokens,\n",
        "    json_extract(payload, '$.output_tokens') as output_tokens,\n",
        "    json_extract(payload, '$.cost') as cost,\n",
        "    json_extract(payload, '$.latency_ms') as latency_ms\n",
        "FROM ingestion_observation_events\n",
        "WHERE json_extract(payload, '$.total_tokens') IS NOT NULL\n",
        "ORDER BY timestamp DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "result8 = pd.read_sql_query(query8, conn)\n",
        "print(\"üìä Query 8: LLM usage metrics from events\")\n",
        "if not result8.empty:\n",
        "    print(result8.to_string(index=False))\n",
        "    \n",
        "    # Calculate totals\n",
        "    if 'cost' in result8.columns and result8['cost'].notna().any():\n",
        "        total_cost = result8['cost'].sum()\n",
        "        total_tokens = result8['total_tokens'].sum()\n",
        "        print(f\"\\nüí∞ Total cost: ${total_cost:.4f}\")\n",
        "        print(f\"üî¢ Total tokens: {total_tokens:,}\")\n",
        "else:\n",
        "    print(\"No LLM metrics found in events\")\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Query 9: LLM costs aggregated by service and model\n",
            "No cost data found\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# Query 9: Aggregate LLM costs by service and model\n",
        "query9 = \"\"\"\n",
        "SELECT \n",
        "    service,\n",
        "    json_extract(payload, '$.model') as model,\n",
        "    COUNT(*) as call_count,\n",
        "    SUM(json_extract(payload, '$.total_tokens')) as total_tokens,\n",
        "    SUM(json_extract(payload, '$.cost')) as total_cost,\n",
        "    AVG(json_extract(payload, '$.latency_ms')) as avg_latency_ms\n",
        "FROM ingestion_observation_events\n",
        "WHERE json_extract(payload, '$.cost') IS NOT NULL\n",
        "GROUP BY service, model\n",
        "ORDER BY total_cost DESC\n",
        "\"\"\"\n",
        "result9 = pd.read_sql_query(query9, conn)\n",
        "print(\"üìä Query 9: LLM costs aggregated by service and model\")\n",
        "if not result9.empty:\n",
        "    print(result9.to_string(index=False))\n",
        "else:\n",
        "    print(\"No cost data found\")\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Using Analytics Views\n",
        "\n",
        "The analytics views module provides pre-built aggregations and summaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Corpus Overview:\n",
            "{'avg_chunks': 2,\n",
            " 'avg_tokens': 139,\n",
            " 'context_strategy_counts': {'document': 3},\n",
            " 'documents': 3,\n",
            " 'risk_counts': {'low': 3},\n",
            " 'total_tokens': 417}\n"
          ]
        }
      ],
      "source": [
        "# Corpus overview - aggregate statistics\n",
        "overview = corpus_overview(store)\n",
        "print(\"üìä Corpus Overview:\")\n",
        "pprint(overview)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Document Size Table:\n",
            "                     document_id                                                                                           source_uri  token_count  chunk_count schema_version context_strategy risk_level\n",
            "eac54e99f2a94080964eafd9c6af1902 C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\documents\\doc3_techcorp_financials.md           83            1          1.0.0         document        low\n",
            "2a6b68c8b120469ea6cb21707ecbfeaf  C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\documents\\doc2_techcorp_expansion.md           92            1          1.0.0         document        low\n",
            "3f857ca539ef4595ae6b2775c23f07ed   C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\documents\\doc1_techcorp_overview.md          242            4          1.0.0         document        low\n"
          ]
        }
      ],
      "source": [
        "# Document size table - per-document metrics\n",
        "doc_table = document_size_table(store)\n",
        "if doc_table:\n",
        "    df_docs = pd.DataFrame(doc_table)\n",
        "    print(\"üìä Document Size Table:\")\n",
        "    print(df_docs.to_string(index=False))\n",
        "else:\n",
        "    print(\"No documents found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Ontology Recommendation Metrics:\n",
            "{'avg_latency_ms': 47462.64520008117,\n",
            " 'by_model': {'unknown': {'avg_latency_ms': 47462.64520008117,\n",
            "                          'calls': 1,\n",
            "                          'input_tokens': 2562,\n",
            "                          'output_tokens': 3967,\n",
            "                          'total_cost': 0.0,\n",
            "                          'total_tokens': 0}},\n",
            " 'by_scope': {'balanced': {'avg_latency_ms': 47462.64520008117,\n",
            "                           'calls': 1,\n",
            "                           'input_tokens': 2562,\n",
            "                           'output_tokens': 3967,\n",
            "                           'total_cost': 0.0,\n",
            "                           'total_tokens': 0}},\n",
            " 'total_calls': 1,\n",
            " 'total_cost': 0.0,\n",
            " 'total_tokens': 0}\n"
          ]
        }
      ],
      "source": [
        "# Ontology recommendation metrics\n",
        "ontology_metrics = ontology_recommendation_metrics(store)\n",
        "print(\"üìä Ontology Recommendation Metrics:\")\n",
        "pprint(ontology_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Triple Extraction Metrics:\n",
            "{'avg_latency_ms': 42010.083333123475,\n",
            " 'by_model': {'unknown': {'avg_latency_ms': 42010.083333123475,\n",
            "                          'avg_triples_per_call': 5.0,\n",
            "                          'calls': 3,\n",
            "                          'entities': 0,\n",
            "                          'input_tokens': 10598,\n",
            "                          'output_tokens': 10320,\n",
            "                          'total_cost': 0.0,\n",
            "                          'total_tokens': 0,\n",
            "                          'triples': 15}},\n",
            " 'by_scope': {'balanced': {'avg_latency_ms': 42010.083333123475,\n",
            "                           'avg_triples_per_call': 5.0,\n",
            "                           'calls': 3,\n",
            "                           'entities': 0,\n",
            "                           'input_tokens': 10598,\n",
            "                           'output_tokens': 10320,\n",
            "                           'total_cost': 0.0,\n",
            "                           'total_tokens': 0,\n",
            "                           'triples': 15}},\n",
            " 'total_calls': 3,\n",
            " 'total_cost': 0.0,\n",
            " 'total_tokens': 0,\n",
            " 'total_triples': 15}\n"
          ]
        }
      ],
      "source": [
        "# Triple extraction metrics\n",
        "triple_metrics = triple_extraction_metrics(store)\n",
        "print(\"üìä Triple Extraction Metrics:\")\n",
        "pprint(triple_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Entity Resolution Metrics:\n",
            "{'edge_matching': {'avg_latency_ms': 10120.650547090918,\n",
            "                   'by_model': {'unknown': {'avg_latency_ms': 10120.650547090918,\n",
            "                                            'calls': 17,\n",
            "                                            'input_tokens': 14690,\n",
            "                                            'output_tokens': 8980,\n",
            "                                            'total_cost': 0.0,\n",
            "                                            'total_tokens': 0}},\n",
            "                   'calls': 17,\n",
            "                   'input_tokens': 14690,\n",
            "                   'output_tokens': 8980,\n",
            "                   'total_cost': 0.0,\n",
            "                   'total_tokens': 0},\n",
            " 'entity_matching': {'avg_latency_ms': 14144.086760096252,\n",
            "                     'by_model': {'unknown': {'avg_latency_ms': 14144.086760096252,\n",
            "                                              'calls': 5,\n",
            "                                              'input_tokens': 5457,\n",
            "                                              'output_tokens': 3480,\n",
            "                                              'total_cost': 0.0,\n",
            "                                              'total_tokens': 0}},\n",
            "                     'calls': 5,\n",
            "                     'input_tokens': 5457,\n",
            "                     'output_tokens': 3480,\n",
            "                     'total_cost': 0.0,\n",
            "                     'total_tokens': 0},\n",
            " 'total_calls': 22,\n",
            " 'total_cost': 0.0,\n",
            " 'total_tokens': 0}\n"
          ]
        }
      ],
      "source": [
        "# Entity resolution metrics\n",
        "resolution_metrics = entity_resolution_metrics(store)\n",
        "print(\"üìä Entity Resolution Metrics:\")\n",
        "pprint(resolution_metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Query 10: Documents ranked by token count with costs\n",
            "                     document_id  token_count  chunk_count  event_count total_cost\n",
            "3f857ca539ef4595ae6b2775c23f07ed          242            4            1       None\n",
            "2a6b68c8b120469ea6cb21707ecbfeaf           92            1            1       None\n",
            "eac54e99f2a94080964eafd9c6af1902           83            1            1       None\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# Query 10: Documents with highest token counts and their associated events\n",
        "query10 = \"\"\"\n",
        "SELECT \n",
        "    o.document_id,\n",
        "    json_extract(o.payload, '$.structural.token_count') as token_count,\n",
        "    json_extract(o.payload, '$.structural.chunk_count') as chunk_count,\n",
        "    COUNT(e.id) as event_count,\n",
        "    SUM(json_extract(e.payload, '$.cost')) as total_cost\n",
        "FROM ingestion_observations o\n",
        "LEFT JOIN ingestion_observation_events e ON o.id = e.observation_id\n",
        "GROUP BY o.id, o.document_id\n",
        "ORDER BY token_count DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "result10 = pd.read_sql_query(query10, conn)\n",
        "print(\"üìä Query 10: Documents ranked by token count with costs\")\n",
        "if not result10.empty:\n",
        "    print(result10.to_string(index=False))\n",
        "else:\n",
        "    print(\"No data found\")\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Query 11: Daily event statistics\n",
            "      date  event_count  unique_services daily_cost\n",
            "2025-11-14            3                1       None\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# Query 11: Time-based analysis - events per day\n",
        "query11 = \"\"\"\n",
        "SELECT \n",
        "    DATE(timestamp) as date,\n",
        "    COUNT(*) as event_count,\n",
        "    COUNT(DISTINCT service) as unique_services,\n",
        "    SUM(json_extract(payload, '$.cost')) as daily_cost\n",
        "FROM ingestion_observation_events\n",
        "GROUP BY DATE(timestamp)\n",
        "ORDER BY date DESC\n",
        "\"\"\"\n",
        "result11 = pd.read_sql_query(query11, conn)\n",
        "print(\"üìä Query 11: Daily event statistics\")\n",
        "if not result11.empty:\n",
        "    print(result11.to_string(index=False))\n",
        "else:\n",
        "    print(\"No events found\")\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Export Data for Further Analysis\n",
        "\n",
        "Export query results to CSV or work with them in pandas DataFrames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Comprehensive Summary DataFrame:\n",
            "Shape: (3, 9)\n",
            "\n",
            "First few rows:\n",
            "                     document_id                ingested_at  token_count  chunk_count                                                                                           source_uri context_strategy  event_count total_cost avg_latency_ms\n",
            "eac54e99f2a94080964eafd9c6af1902 2025-11-14 22:18:11.425474           83            1 C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\documents\\doc3_techcorp_financials.md         document            1       None           None\n",
            "2a6b68c8b120469ea6cb21707ecbfeaf 2025-11-14 22:18:11.327198           92            1  C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\documents\\doc2_techcorp_expansion.md         document            1       None           None\n",
            "3f857ca539ef4595ae6b2775c23f07ed 2025-11-14 22:18:11.233879          242            4   C:\\Users\\danie\\Repos\\spindle\\spindle\\notebooks\\example_project\\documents\\doc1_techcorp_overview.md         document            1       None           None\n",
            "\n",
            "üìà Statistics:\n",
            "  Total documents: 3\n",
            "  Total tokens: 417\n",
            "  Average tokens per document: 139\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(str(ANALYTICS_DB_PATH))\n",
        "\n",
        "# Create a comprehensive summary DataFrame\n",
        "summary_query = \"\"\"\n",
        "SELECT \n",
        "    o.document_id,\n",
        "    o.ingested_at,\n",
        "    json_extract(o.payload, '$.structural.token_count') as token_count,\n",
        "    json_extract(o.payload, '$.structural.chunk_count') as chunk_count,\n",
        "    json_extract(o.payload, '$.metadata.source_uri') as source_uri,\n",
        "    json_extract(o.payload, '$.context.recommended_strategy') as context_strategy,\n",
        "    COUNT(e.id) as event_count,\n",
        "    SUM(json_extract(e.payload, '$.cost')) as total_cost,\n",
        "    AVG(json_extract(e.payload, '$.latency_ms')) as avg_latency_ms\n",
        "FROM ingestion_observations o\n",
        "LEFT JOIN ingestion_observation_events e ON o.id = e.observation_id\n",
        "GROUP BY o.id\n",
        "ORDER BY o.ingested_at DESC\n",
        "\"\"\"\n",
        "df_summary = pd.read_sql_query(summary_query, conn)\n",
        "conn.close()\n",
        "\n",
        "print(\"üìä Comprehensive Summary DataFrame:\")\n",
        "print(f\"Shape: {df_summary.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df_summary.head().to_string(index=False))\n",
        "\n",
        "# Display basic statistics\n",
        "if not df_summary.empty:\n",
        "    print(f\"\\nüìà Statistics:\")\n",
        "    print(f\"  Total documents: {len(df_summary)}\")\n",
        "    if 'token_count' in df_summary.columns and df_summary['token_count'].notna().any():\n",
        "        print(f\"  Total tokens: {df_summary['token_count'].sum():,}\")\n",
        "        print(f\"  Average tokens per document: {df_summary['token_count'].mean():.0f}\")\n",
        "    if 'total_cost' in df_summary.columns and df_summary['total_cost'].notna().any():\n",
        "        print(f\"  Total cost: ${df_summary['total_cost'].sum():.4f}\")\n",
        "    \n",
        "    # Optionally export to CSV\n",
        "    # df_summary.to_csv('analytics_summary.csv', index=False)\n",
        "    # print(\"\\n‚úÖ Exported to analytics_summary.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "‚úÖ **Database Connection**: Connected to analytics SQLite database  \n",
        "‚úÖ **Schema Exploration**: Examined table structures  \n",
        "‚úÖ **API Queries**: Used AnalyticsStore methods to fetch observations and events  \n",
        "‚úÖ **Direct SQL**: Performed SQL queries for detailed analysis  \n",
        "‚úÖ **JSON Extraction**: Extracted nested JSON data from payloads  \n",
        "‚úÖ **Aggregations**: Calculated statistics and summaries  \n",
        "‚úÖ **Analytics Views**: Used pre-built view functions  \n",
        "‚úÖ **Data Export**: Created DataFrames for further analysis  \n",
        "\n",
        "### Key Tables\n",
        "\n",
        "- `ingestion_observations`: Document observations with JSON payloads\n",
        "- `ingestion_observation_events`: Service events linked to observations\n",
        "\n",
        "### Common Query Patterns\n",
        "\n",
        "1. **Count observations**: `SELECT COUNT(*) FROM ingestion_observations`\n",
        "2. **Extract JSON fields**: `json_extract(payload, '$.structural.token_count')`\n",
        "3. **Join observations with events**: `LEFT JOIN ingestion_observation_events`\n",
        "4. **Aggregate by service**: `GROUP BY service`\n",
        "5. **Time-based analysis**: `DATE(timestamp)` for daily aggregations\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore specific document observations in detail\n",
        "- Analyze LLM cost trends over time\n",
        "- Compare metrics across different document types\n",
        "- Build custom visualizations using pandas/matplotlib\n",
        "- Integrate with the Streamlit dashboard for interactive exploration\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
