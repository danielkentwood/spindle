{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Document Ingestion Walkthrough\n",
        "\n",
        "This notebook demonstrates how to route raw documents through the Spindle ingestion pipeline. You'll configure templates, build a pipeline, ingest sample files, and inspect the resulting artifacts, metrics, and graph.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting Started\n",
        "\n",
        "- Ensure the repository dependencies are installed (for example via `uv pip install -e \".[dev]\"`).\n",
        "- Make sure you can import `spindle` and the LangChain components referenced by the default templates.\n",
        "- Run the cells sequentially; each step builds on the previous one.\n",
        "\n",
        "### Walkthrough roadmap\n",
        "\n",
        "1. Inspect default ingestion templates\n",
        "2. Prepare sample documents\n",
        "3. Build an ingestion pipeline\n",
        "4. Execute ingestion\n",
        "5. Explore artifacts, chunks, graphs, and metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working in: /Users/thalamus/Repos/spindle/spindle/notebooks\n",
            "Demo documents will be written to: /Users/thalamus/Repos/spindle/spindle/notebooks/spindle/notebooks/_doc_ingestion_demo\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import asdict\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import shutil\n",
        "import textwrap\n",
        "\n",
        "from spindle.ingestion import (\n",
        "    DEFAULT_TEMPLATE_SPECS,\n",
        "    IngestionConfig,\n",
        "    TemplateRegistry,\n",
        "    build_ingestion_pipeline,\n",
        ")\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "DATA_DIR = BASE_DIR / \"_doc_ingestion_demo\"\n",
        "\n",
        "if DATA_DIR.exists():\n",
        "    shutil.rmtree(DATA_DIR)\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Working in: {BASE_DIR}\")\n",
        "print(f\"Demo documents will be written to: {DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Inspect the default templates\n",
        "\n",
        "The ingestion pipeline picks a `TemplateSpec` based on file metadata. The default templates cover common text and PDF files. Let's examine what is registered out of the box.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2 template(s):\n",
            "\n",
            "- default-text\n",
            "  description : Plain text and Markdown documents\n",
            "  loader      : langchain_community.document_loaders.TextLoader\n",
            "  selector    : extensions=.txt, .md, .mdx, .rst\n",
            "\n",
            "- default-pdf\n",
            "  description : PDF documents with layout-aware parsing\n",
            "  loader      : langchain_community.document_loaders.PDFMinerLoader\n",
            "  selector    : extensions=.pdf\n",
            "\n"
          ]
        }
      ],
      "source": [
        "registry = TemplateRegistry(DEFAULT_TEMPLATE_SPECS)\n",
        "\n",
        "print(f\"Loaded {len(registry)} template(s):\\n\")\n",
        "for spec in registry:\n",
        "    selector = spec.selector\n",
        "    print(f\"- {spec.name}\")\n",
        "    print(f\"  description : {spec.description or '—'}\")\n",
        "    print(f\"  loader      : {spec.loader}\")\n",
        "    print(\n",
        "        \"  selector    : extensions=\"\n",
        "        f\"{', '.join(selector.file_extensions) or '—'}\"\n",
        "    )\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare sample documents\n",
        "\n",
        "We'll fabricate a small knowledge base with two Markdown files so the pipeline has something to ingest. Each file uses headings and bullet points to simulate real content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created demo document: spindle/notebooks/_doc_ingestion_demo/product_overview.md\n",
            "Created demo document: spindle/notebooks/_doc_ingestion_demo/faq.md\n",
            "\n",
            "Documents queued for ingestion:\n",
            "- /Users/thalamus/Repos/spindle/spindle/notebooks/spindle/notebooks/_doc_ingestion_demo/product_overview.md\n",
            "- /Users/thalamus/Repos/spindle/spindle/notebooks/spindle/notebooks/_doc_ingestion_demo/faq.md\n"
          ]
        }
      ],
      "source": [
        "sample_docs = {\n",
        "    \"product_overview.md\": \"\"\"\n",
        "        # Spindle Product Overview\n",
        "\n",
        "        ## What problem does Spindle solve?\n",
        "        - Automates extracting structured knowledge from unstructured text.\n",
        "        - Produces chunk-level artifacts primed for retrieval and graph reasoning.\n",
        "\n",
        "        ## Key capabilities\n",
        "        - Template-driven ingestion\n",
        "        - Document graph construction\n",
        "        - Event hooks for observability\n",
        "        - Extensible metadata enrichment\n",
        "\n",
        "        ## Next steps\n",
        "        1. Ingest documents\n",
        "        2. Build embeddings\n",
        "        3. Integrate with downstream agents\n",
        "        \"\"\",\n",
        "    \"faq.md\": \"\"\"\n",
        "        # Frequently Asked Questions\n",
        "\n",
        "        ## How does chunking work?\n",
        "        Spindle relies on LangChain splitters configured per template. You can\n",
        "        override chunk sizes, separators, or even switch to semantic splitters.\n",
        "\n",
        "        ## Where are ingestion metrics stored?\n",
        "        Metrics are returned with every run. Optionally, connect a document catalog\n",
        "        or vector store to persist them.\n",
        "\n",
        "        ## Can I add custom metadata?\n",
        "        Yes! Provide metadata extractor callables in your template to populate\n",
        "        document-level context (titles, tags, owners, etc.).\n",
        "        \"\"\",\n",
        "}\n",
        "\n",
        "documents_to_ingest = []\n",
        "for name, body in sample_docs.items():\n",
        "    path = DATA_DIR / name\n",
        "    path.write_text(textwrap.dedent(body).strip() + \"\\n\", encoding=\"utf-8\")\n",
        "    documents_to_ingest.append(path)\n",
        "    print(f\"Created demo document: {path.relative_to(BASE_DIR)}\")\n",
        "\n",
        "print(\"\\nDocuments queued for ingestion:\")\n",
        "for path in documents_to_ingest:\n",
        "    print(f\"- {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build the ingestion pipeline\n",
        "\n",
        "The pipeline combines an `IngestionConfig` with a `TemplateRegistry`. The default configuration listed above is enough for our Markdown examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline ready.\n",
            "Templates registered: ['default-text', 'default-pdf']\n"
          ]
        }
      ],
      "source": [
        "config = IngestionConfig(template_specs=DEFAULT_TEMPLATE_SPECS)\n",
        "pipeline = build_ingestion_pipeline(config=config, registry=registry)\n",
        "\n",
        "print(\"Pipeline ready.\")\n",
        "print(f\"Templates registered: {[spec.name for spec in registry]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Execute ingestion\n",
        "\n",
        "Call `pipeline.ingest(paths=...)` with a list of `Path` objects. The run returns artifacts, chunk records, events, and metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ingestion complete.\n",
            "Documents processed: 2\n",
            "Chunks produced   : 2\n"
          ]
        }
      ],
      "source": [
        "result = pipeline.ingest(paths=documents_to_ingest)\n",
        "\n",
        "print(\"Ingestion complete.\")\n",
        "print(f\"Documents processed: {len(result.documents)}\")\n",
        "print(f\"Chunks produced   : {len(result.chunks)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Explore the results\n",
        "\n",
        "Start with the run metrics to understand what happened during ingestion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started at          : 2025-11-07 22:51:11\n",
            "Finished at         : 2025-11-07 22:51:11\n",
            "Processed documents : 2\n",
            "Processed chunks    : 2\n",
            "Bytes read          : 946\n",
            "Errors              : —\n",
            "\n",
            "Stage durations (ms):\n",
            "- checksum     0.20\n",
            "- load         137.59\n",
            "- preprocess   0.01\n",
            "- split        0.19\n",
            "- metadata     0.01\n",
            "- chunks       0.05\n",
            "- graph        0.04\n",
            "\n",
            "Stage call counts:\n",
            "- checksum     2\n",
            "- load         2\n",
            "- preprocess   2\n",
            "- split        2\n",
            "- metadata     2\n",
            "- chunks       2\n",
            "- graph        2\n"
          ]
        }
      ],
      "source": [
        "metrics = result.metrics\n",
        "\n",
        "print(f\"Started at          : {metrics.started_at:%Y-%m-%d %H:%M:%S}\")\n",
        "print(f\"Finished at         : {metrics.finished_at:%Y-%m-%d %H:%M:%S}\")\n",
        "print(f\"Processed documents : {metrics.processed_documents}\")\n",
        "print(f\"Processed chunks    : {metrics.processed_chunks}\")\n",
        "print(f\"Bytes read          : {metrics.bytes_read}\")\n",
        "print(f\"Errors              : {metrics.errors or '—'}\\n\")\n",
        "\n",
        "print(\"Stage durations (ms):\")\n",
        "for stage, duration in metrics.extra.get(\"stage_durations_ms\", {}).items():\n",
        "    print(f\"- {stage:<12} {duration:.2f}\")\n",
        "\n",
        "print(\"\\nStage call counts:\")\n",
        "for stage, count in metrics.extra.get(\"stage_calls\", {}).items():\n",
        "    print(f\"- {stage:<12} {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Document artifacts\n",
        "\n",
        "Each `DocumentArtifact` captures high-level metadata about a source file. The raw bytes are available if you need to persist or reprocess the original content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'checksum': 'ce9069f071b88346fc1c5c687f05a51c08081ed6a8a21cd9af1f7cf71bb12e2e',\n",
            " 'created_at': datetime.datetime(2025, 11, 7, 22, 51, 11, 204714),\n",
            " 'document_id': 'a6f75dcf673149b0b9045bb9f6a4b45c',\n",
            " 'loader_name': 'langchain_community.document_loaders.TextLoader',\n",
            " 'metadata': {},\n",
            " 'raw_bytes': '442 bytes',\n",
            " 'source_path': PosixPath('/Users/thalamus/Repos/spindle/spindle/notebooks/spindle/notebooks/_doc_ingestion_demo/product_overview.md'),\n",
            " 'template_name': 'default-text'}\n",
            "\n",
            "{'checksum': '8fce55c63acf05dfc68f97c0fbf326d32f0c256207dea7d4e8a508111a5819e2',\n",
            " 'created_at': datetime.datetime(2025, 11, 7, 22, 51, 11, 212722),\n",
            " 'document_id': 'a0c002723159470ebafe1106fb78fe35',\n",
            " 'loader_name': 'langchain_community.document_loaders.TextLoader',\n",
            " 'metadata': {},\n",
            " 'raw_bytes': '504 bytes',\n",
            " 'source_path': PosixPath('/Users/thalamus/Repos/spindle/spindle/notebooks/spindle/notebooks/_doc_ingestion_demo/faq.md'),\n",
            " 'template_name': 'default-text'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for artifact in result.documents:\n",
        "    artifact_dict = asdict(artifact)\n",
        "    raw_bytes = artifact_dict.pop(\"raw_bytes\", None)\n",
        "    artifact_dict[\"raw_bytes\"] = f\"{len(raw_bytes)} bytes\" if raw_bytes else \"—\"\n",
        "    pprint(artifact_dict)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunk artifacts\n",
        "\n",
        "Chunks are suitable for vector stores or downstream retrieval. Below we preview the first few characters of each chunk to confirm the splitter configuration behaved as expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 1\n",
            "  chunk_id   : b6b4007997574adda39ddf6b900dbf68\n",
            "  document_id: a6f75dcf673149b0b9045bb9f6a4b45c\n",
            "  metadata   : {'source': '/Users/thalamus/Repos/spindle/spindle/notebooks/spindle/notebooks/_doc_ingestion_demo/product_overview.md', 'document_id': 'a6f75dcf673149b0b9045bb9f6a4b45c'}\n",
            "  preview    : # Spindle Product Overview  ## What problem does Spindle solve? - Automates extracting structured knowledge from unstructured text. - Produces chunk-level artif...\n",
            "\n",
            "Chunk 2\n",
            "  chunk_id   : e76222dfd0eb4c0f823bb37a5a46415b\n",
            "  document_id: a0c002723159470ebafe1106fb78fe35\n",
            "  metadata   : {'source': '/Users/thalamus/Repos/spindle/spindle/notebooks/spindle/notebooks/_doc_ingestion_demo/faq.md', 'document_id': 'a0c002723159470ebafe1106fb78fe35'}\n",
            "  preview    : # Frequently Asked Questions  ## How does chunking work? Spindle relies on LangChain splitters configured per template. You can override chunk sizes, separators...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for index, chunk in enumerate(result.chunks, start=1):\n",
        "    preview = chunk.text.replace(\"\\n\", \" \")[:160]\n",
        "    print(f\"Chunk {index}\")\n",
        "    print(f\"  chunk_id   : {chunk.chunk_id}\")\n",
        "    print(f\"  document_id: {chunk.document_id}\")\n",
        "    print(f\"  metadata   : {chunk.metadata}\")\n",
        "    print(f\"  preview    : {preview}...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Document graph snapshot\n",
        "\n",
        "The pipeline also builds a `DocumentGraph` that links documents to their chunks. More advanced templates can add extra relationships (for example, linking glossary terms or headings).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph nodes : 4\n",
            "Graph edges : 2\n",
            "\n",
            "Sample nodes:\n",
            "{'attributes': {},\n",
            " 'document_id': 'a6f75dcf673149b0b9045bb9f6a4b45c',\n",
            " 'label': 'product_overview.md',\n",
            " 'node_id': 'doc::a6f75dcf673149b0b9045bb9f6a4b45c'}\n",
            "\n",
            "{'attributes': {'document_id': 'a6f75dcf673149b0b9045bb9f6a4b45c',\n",
            "                'source': '/Users/thalamus/Repos/spindle/spindle/notebooks/spindle/notebooks/_doc_ingestion_demo/product_overview.md'},\n",
            " 'document_id': 'a6f75dcf673149b0b9045bb9f6a4b45c',\n",
            " 'label': 'Chunk 0',\n",
            " 'node_id': 'chunk::b6b4007997574adda39ddf6b900dbf68'}\n",
            "\n",
            "{'attributes': {},\n",
            " 'document_id': 'a0c002723159470ebafe1106fb78fe35',\n",
            " 'label': 'faq.md',\n",
            " 'node_id': 'doc::a0c002723159470ebafe1106fb78fe35'}\n",
            "\n",
            "{'attributes': {'document_id': 'a0c002723159470ebafe1106fb78fe35',\n",
            "                'source': '/Users/thalamus/Repos/spindle/spindle/notebooks/spindle/notebooks/_doc_ingestion_demo/faq.md'},\n",
            " 'document_id': 'a0c002723159470ebafe1106fb78fe35',\n",
            " 'label': 'Chunk 0',\n",
            " 'node_id': 'chunk::e76222dfd0eb4c0f823bb37a5a46415b'}\n",
            "\n",
            "Sample edges:\n",
            "{'attributes': {},\n",
            " 'edge_id': 'doc-chunk::doc::a6f75dcf673149b0b9045bb9f6a4b45c->chunk::b6b4007997574adda39ddf6b900dbf68',\n",
            " 'relation': 'has_chunk',\n",
            " 'source_id': 'doc::a6f75dcf673149b0b9045bb9f6a4b45c',\n",
            " 'target_id': 'chunk::b6b4007997574adda39ddf6b900dbf68'}\n",
            "\n",
            "{'attributes': {},\n",
            " 'edge_id': 'doc-chunk::doc::a0c002723159470ebafe1106fb78fe35->chunk::e76222dfd0eb4c0f823bb37a5a46415b',\n",
            " 'relation': 'has_chunk',\n",
            " 'source_id': 'doc::a0c002723159470ebafe1106fb78fe35',\n",
            " 'target_id': 'chunk::e76222dfd0eb4c0f823bb37a5a46415b'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "graph = result.document_graph\n",
        "\n",
        "print(f\"Graph nodes : {len(graph.nodes)}\")\n",
        "print(f\"Graph edges : {len(graph.edges)}\\n\")\n",
        "\n",
        "print(\"Sample nodes:\")\n",
        "for node in graph.nodes[:6]:\n",
        "    pprint(asdict(node))\n",
        "    print()\n",
        "\n",
        "print(\"Sample edges:\")\n",
        "for edge in graph.edges[:6]:\n",
        "    pprint(asdict(edge))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Emitted events\n",
        "\n",
        "Every pipeline stage pushes `IngestionEvent` records that you can stream to observers for logging, metrics, or tracing. Here we echo them inline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'checksum'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'checksum', 'duration_ms': 0.12024299940094352}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'load'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'load', 'duration_ms': 137.5019010156393}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'preprocess'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'preprocess', 'duration_ms': 0.00528499367646873}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'split'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'split', 'duration_ms': 0.13472599675878882}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'metadata'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'metadata', 'duration_ms': 0.004964007530361414}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'chunks'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'chunks', 'duration_ms': 0.026021996745839715}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'graph'}\n",
            "2025-11-07 22:51:11 | graph_built     | {'document_id': 'a6f75dcf673149b0b9045bb9f6a4b45c', 'chunk_count': 1}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'graph', 'duration_ms': 0.02414401387795806}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'checksum'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'checksum', 'duration_ms': 0.08384700049646199}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'load'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'load', 'duration_ms': 0.08761198841966689}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'preprocess'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'preprocess', 'duration_ms': 0.003602996002882719}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'split'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'split', 'duration_ms': 0.05736900493502617}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'metadata'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'metadata', 'duration_ms': 0.0043539912439882755}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'chunks'}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'chunks', 'duration_ms': 0.022661988623440266}\n",
            "2025-11-07 22:51:11 | stage_start     | {'stage': 'graph'}\n",
            "2025-11-07 22:51:11 | graph_built     | {'document_id': 'a0c002723159470ebafe1106fb78fe35', 'chunk_count': 1}\n",
            "2025-11-07 22:51:11 | stage_complete  | {'stage': 'graph', 'duration_ms': 0.017230981029570103}\n"
          ]
        }
      ],
      "source": [
        "for event in result.events:\n",
        "    print(f\"{event.timestamp:%Y-%m-%d %H:%M:%S} | {event.name:<15} | {event.payload}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Add or override templates with `TemplateSpec` instances tailored to your corpus.\n",
        "- Connect a document catalog or vector store (for example Chroma) when you want results persisted automatically.\n",
        "- Attach observers to `build_ingestion_pipeline` to feed events into your logging or monitoring stack.\n",
        "- Extend metadata extractors to enrich document context before indexing.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
