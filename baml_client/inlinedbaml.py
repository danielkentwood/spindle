# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Using the new OpenAI Responses API for enhanced formatting\nclient<llm> CustomGPT5 {\n  provider openai-responses\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT5Mini {\n  provider openai-responses\n  retry_policy Exponential\n  options {\n    model \"gpt-5-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Openai with chat completion\nclient<llm> CustomGPT5Chat {\n  provider openai\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Latest Anthropic Claude 4 models\nclient<llm> CustomOpus4 {\n  provider anthropic\n  options {\n    model \"claude-opus-4-1-20250805\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet4 {\n  provider anthropic\n  options {\n    model \"claude-sonnet-4-20250514\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-5-haiku-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// Example Google AI client (uncomment to use)\n// client<llm> CustomGemini {\n//   provider google-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     api_key env.GOOGLE_API_KEY\n//   }\n// }\n\n// Example AWS Bedrock client (uncomment to use)\n// client<llm> CustomBedrock {\n//   provider aws-bedrock\n//   options {\n//     model \"anthropic.claude-sonnet-4-20250514-v1:0\"\n//     region \"us-east-1\"\n//     // AWS credentials are auto-detected from env vars\n//   }\n// }\n\n// Example Azure OpenAI client (uncomment to use)\n// client<llm> CustomAzure {\n//   provider azure-openai\n//   options {\n//     model \"gpt-5\"\n//     api_key env.AZURE_OPENAI_API_KEY\n//     base_url \"https://MY_RESOURCE_NAME.openai.azure.com/openai/deployments/MY_DEPLOYMENT_ID\"\n//     api_version \"2024-10-01-preview\"\n//   }\n// }\n\n// Example Vertex AI client (uncomment to use)\n// client<llm> CustomVertex {\n//   provider vertex-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     location \"us-central1\"\n//     // Uses Google Cloud Application Default Credentials\n//   }\n// }\n\n// Example Ollama client for local models (uncomment to use)\n// client<llm> CustomOllama {\n//   provider openai-generic\n//   options {\n//     base_url \"http://localhost:11434/v1\"\n//     model \"llama4\"\n//     default_role \"user\" // Most local models prefer the user role\n//     // No API key needed for local Ollama\n//   }\n// }\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT5Mini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT5Mini, CustomGPT5]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.211.2\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // You can also use custom LLM params with a custom client name from clients.baml like \"client CustomGPT5\" or \"client CustomSonnet4\"\n  client \"openai-responses/gpt-5-mini\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "spindle.baml": "// Spindle: Knowledge Graph Triple Extraction\n// This file defines the data structures and extraction function for building knowledge graphs from text\n\n// Source metadata for tracking where a triple came from\nclass SourceMetadata {\n  source_name string @description(\"The name or identifier of the source document\")\n  source_url string? @description(\"Optional URL of the source document\")\n}\n\n// Character span indicating text evidence for a triple\nclass CharacterSpan {\n  text string @description(\"The exact text from the source that supports this triple\")\n  start int? @description(\"Starting character index in the source text (0-based, computed in post-processing)\")\n  end int? @description(\"Ending character index in the source text (exclusive, computed in post-processing)\")\n}\n\n// A triple represents a subject-predicate-object relationship with supporting evidence\nclass Triple {\n  subject string @description(\"The subject entity of the triple\")\n  predicate string @description(\"The relationship/predicate connecting subject and object\")\n  object string @description(\"The object entity of the triple\")\n  source SourceMetadata @description(\"Metadata about the source of this triple\")\n  supporting_spans CharacterSpan[] @description(\"Character spans in the source text that support this triple\")\n  extraction_datetime string? @description(\"ISO 8601 datetime when this triple was extracted (set automatically in post-processing)\")\n}\n\n// Defines a type of entity that can appear in the knowledge graph\nclass EntityType {\n  name string @description(\"The name of the entity type (e.g., 'Person', 'Organization')\")\n  description string @description(\"A description of what this entity type represents\")\n}\n\n// Defines a type of relationship between entities\nclass RelationType {\n  name string @description(\"The name of the relation type (e.g., 'works_at', 'located_in')\")\n  description string @description(\"A description of what this relation represents\")\n  domain string @description(\"The entity type that can be the subject of this relation\")\n  range string @description(\"The entity type that can be the object of this relation\")\n}\n\n// An ontology defines the structure of the knowledge graph\nclass Ontology {\n  entity_types EntityType[] @description(\"List of valid entity types\")\n  relation_types RelationType[] @description(\"List of valid relation types\")\n}\n\n// The result of triple extraction\nclass ExtractionResult {\n  triples Triple[] @description(\"List of extracted triples from the text\")\n  reasoning string @description(\"Explanation of the extraction decisions and entity consistency choices\")\n}\n\n// Result of ontology recommendation\nclass OntologyRecommendation {\n  ontology Ontology @description(\"The recommended ontology with entity and relation types\")\n  text_purpose string @description(\"Analysis of the text's overarching purpose or goal\")\n  reasoning string @description(\"Explanation of why these entity and relation types were recommended\")\n}\n\n// Result of ontology extension analysis\nclass OntologyExtension {\n  needs_extension bool @description(\"Whether the ontology needs to be extended for this text\")\n  new_entity_types EntityType[] @description(\"New entity types to add (empty if none needed)\")\n  new_relation_types RelationType[] @description(\"New relation types to add (empty if none needed)\")\n  critical_information_at_risk string @description(\"Description of what critical information would be lost without extension\")\n  reasoning string @description(\"Detailed explanation of why extension is or isn't needed\")\n}\n\n// Main function to extract knowledge graph triples from text\nfunction ExtractTriples(\n  text: string,\n  ontology: Ontology,\n  source_metadata: SourceMetadata,\n  existing_triples: Triple[]\n) -> ExtractionResult {\n  client CustomSonnet4\n  prompt #\"\n    You are a knowledge graph extraction expert. Your task is to extract structured triples (subject-predicate-object) from the provided text, along with supporting evidence.\n\n    {{ _.role(\"user\") }}\n    \n    ONTOLOGY:\n    You must extract triples that conform to the following ontology:\n    \n    Valid Entity Types:\n    {% for entity_type in ontology.entity_types %}\n    - {{ entity_type.name }}: {{ entity_type.description }}\n    {% endfor %}\n    \n    Valid Relation Types:\n    {% for relation_type in ontology.relation_types %}\n    - {{ relation_type.name }}: {{ relation_type.description }}\n      (Domain: {{ relation_type.domain }}, Range: {{ relation_type.range }})\n    {% endfor %}\n    \n    SOURCE METADATA:\n    Source Name: {{ source_metadata.source_name }}\n    {% if source_metadata.source_url %}\n    Source URL: {{ source_metadata.source_url }}\n    {% endif %}\n    \n    EXISTING TRIPLES:\n    {% if existing_triples|length > 0 %}\n    The following triples have already been extracted from OTHER sources. You MUST:\n    1. Use consistent entity names - if an entity appears in existing triples, use the EXACT same name\n    2. Maintain entity identity - recognize when entities in the new text are the same as entities in existing triples\n    3. Duplicate triples ARE ALLOWED if they come from different sources (different source names)\n    4. If you extract the same fact that exists in existing triples, it's okay as long as it's from this new source\n    \n    {% for triple in existing_triples %}\n    - {{ triple.subject }} -> {{ triple.predicate }} -> {{ triple.object }} (from: {{ triple.source.source_name }})\n    {% endfor %}\n    {% else %}\n    This is the first extraction, so there are no existing triples to consider.\n    {% endif %}\n    \n    TEXT TO ANALYZE:\n    {{ text }}\n    \n    INSTRUCTIONS:\n    1. Extract all meaningful triples from the text that conform to the ontology\n    2. Only use entity types and relation types defined in the ontology\n    3. Use clear, consistent entity names (e.g., \"John Smith\" not \"John\" or \"Smith\")\n    4. If an entity appears in existing triples, use the exact same name for consistency\n    5. For each triple, identify the TEXT SPANS that provide evidence for the triple\n       - Copy the EXACT text from the source that supports the triple\n       - You can include multiple text spans if the evidence is spread across different parts of the text\n       - Be precise - copy the text exactly as it appears, including punctuation and spacing\n       - Include enough context to make the evidence clear\n    6. Set the source metadata for each triple to the provided source information\n    7. Provide reasoning explaining your extraction decisions, entity consistency choices, and how you identified supporting spans\n    \n    EXAMPLE of text spans:\n    If the text is \"Alice works at TechCorp in San Francisco.\" and you extract the triple (Alice, works_at, TechCorp),\n    the supporting span text would be: \"Alice works at TechCorp\"\n    \n    Note: You only need to provide the text content. Character indices will be computed automatically in post-processing.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Function to recommend an ontology based on text analysis\nfunction RecommendOntology(\n  text: string,\n  scope: string\n) -> OntologyRecommendation {\n  client CustomSonnet4\n  prompt #\"\n    You are a knowledge graph ontology design expert. Your task is to analyze the provided text, understand its overarching purpose and domain, and recommend an appropriate ontology (entity types and relation types) that would be suitable for extracting knowledge from this and similar texts.\n\n    {{ _.role(\"user\") }}\n    \n    TEXT TO ANALYZE:\n    {{ text }}\n    \n    ONTOLOGY SCOPE: {{ scope }}\n    \n    GRANULARITY GUIDELINES:\n    \n    Your goal is to design an ontology with {{ scope }} scope. Use these principles to guide your decisions about how many types to create and how granular to make them.\n    \n    ENTITY TYPE GRANULARITY PRINCIPLES:\n    \n    1. **Abstraction Level**: Choose the right level of abstraction\n       - Too broad: \"Thing\", \"Entity\" (not useful)\n       - Too narrow: \"SoftwareEngineerAtGoogle\", \"CEOOfTechStartup\" (overfitting)\n       - Just right: \"Person\", \"Organization\", \"JobRole\" (reusable across instances)\n    \n    2. **Domain Relevance**: Include entity types that are central to THIS domain\n       - For medical text: Patient, Medication, Condition, Procedure, Hospital\n       - For business text: Company, Person, Investment, Product, Market\n       - Don't include types that rarely appear or aren't relevant to this specific text\n    \n    3. **Distinctiveness Test**: Only create separate types if they have:\n       - Different attributes or characteristics\n       - Different relationship patterns\n       - Different analytical value\n       - Example: \"Professor\" and \"Student\" are distinct if their relationships differ\n       - Example: \"CEO\" and \"CTO\" might both be \"ExecutiveRole\" if relationships are similar\n    \n    4. **Generalization**: Combine similar concepts under broader types when appropriate\n       - Instead of: \"Dog\", \"Cat\", \"Bird\" → use \"Animal\" (if distinctions don't matter)\n       - Instead of: \"Laptop\", \"Desktop\", \"Tablet\" → use \"Device\" or \"Computer\"\n       - BUT: Keep separate if the distinction is analytically important for this domain\n    \n    5. **Coverage**: Ensure types cover the main concepts in the text\n       - Don't create types for concepts that appear only once\n       - DO create types for recurring or structurally important concepts\n       - Focus on concepts that are central to understanding this domain\n    \n    RELATION TYPE GRANULARITY PRINCIPLES:\n    \n    1. **Semantic Precision**: Relations should capture meaningful distinctions\n       - Too vague: \"related_to\", \"associated_with\" (not informative)\n       - Too specific: \"works_at_as_senior_engineer_since_2020\" (overfitting)\n       - Just right: \"works_at\", \"employed_by\", \"manages\" (clear semantics)\n    \n    2. **Directionality Matters**: Choose direction that reflects real-world semantics\n       - \"Person works_at Organization\" (natural)\n       - Not \"Organization employs Person\" (unless that's the focus)\n       - Be consistent in your directional choices\n    \n    3. **Relationship Patterns**: Include relations that appear or could appear multiple times\n       - Medical: \"treats\", \"prescribes\", \"diagnoses\", \"hospitalized_at\"\n       - Business: \"invests_in\", \"founded\", \"acquired_by\", \"partners_with\"\n       - Focus on the relationships that matter for this domain\n    \n    4. **Avoid Redundancy**: Don't create near-synonyms unless there's semantic distinction\n       - Don't have both \"works_at\" and \"employed_by\" (pick one)\n       - Don't have both \"located_in\" and \"based_in\" (pick one)\n       - DO have \"founded\" and \"acquired\" (different events with different meanings)\n    \n    5. **Completeness**: Capture the key relationship types in the domain\n       - Think: \"What questions would someone ask about this data?\"\n       - Include relations that support those analytical questions\n       - Every relation should serve a clear analytical purpose\n    \n    SCOPE-SPECIFIC GUIDANCE:\n    \n    **If scope is \"minimal\":**\n    - Create only the most essential entity types (core concepts only)\n    - Focus on the most frequent and important relationships\n    - Err on the side of broader categories that combine similar concepts\n    - Typical result: 3-8 entity types, 4-10 relation types\n    - Use case: Quick extraction, simple queries, identifying broad patterns\n    - Example: For a business article → Person, Organization, Location, works_at, located_in\n    \n    **If scope is \"balanced\":**\n    - Include entity types for all significant concepts in the domain\n    - Capture the main relationship patterns without over-specifying\n    - Balance specificity with reusability across similar texts\n    - Typical result: 6-12 entity types, 8-15 relation types\n    - Use case: Standard analysis, general-purpose extraction, most common scenarios\n    - Example: For a business article → Person, Organization, Location, Product, Investment, works_at, located_in, founded, invests_in, develops\n    \n    **If scope is \"comprehensive\":**\n    - Include entity types for all distinct and meaningful concepts\n    - Capture nuanced relationship types that reflect domain expertise\n    - Allow for more domain-specific and specialized types\n    - Typical result: 10-20 entity types, 12-25 relation types\n    - Use case: Detailed analysis, domain expertise, research, specialized queries\n    - Example: For a business article → Person, Organization, Location, Product, Investment, Technology, Market, Role, works_at, located_in, founded, invests_in, develops, competes_with, targets, acquired_by, partners_with\n    \n    QUALITY OVER QUANTITY:\n    - It's better to have fewer, well-defined types than many poorly-defined ones\n    - Each type should have a clear, distinct purpose and detailed description\n    - Each type should be distinguishable from others in meaningful ways\n    - Every type you create should be useful for analysis and querying\n    - The numbers above are guidelines, not requirements - use your judgment\n    \n    SELF-CHECK QUESTIONS:\n    Before finalizing your ontology, ask yourself:\n    1. Can I clearly explain when to use each entity type vs. others?\n    2. Would two different people classify entities consistently with these types?\n    3. Are my relation types capturing distinct semantic relationships?\n    4. Can I extract useful, actionable information with this ontology?\n    5. Is this ontology reusable for similar texts in this domain?\n    6. Does the granularity match the requested scope level?\n    \n    ONTOLOGY DESIGN PRINCIPLES:\n    - Entity types should be nouns representing concrete or abstract concepts\n    - Relation types should be verbs or verb phrases that connect entities\n    - Each relation type must specify its domain (subject entity type) and range (object entity type)\n    - Prefer commonly understood terms over jargon unless domain-specific terms are essential\n    - Think about what would be useful for querying and analyzing this type of text\n    - Ensure descriptions are clear and detailed enough for accurate extraction\n    \n    Your task is to analyze THIS specific text and recommend the most appropriate ontology for it,\n    following the principles above and designing for the requested \"{{ scope }}\" scope level.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Function to analyze if an existing ontology needs extension for new text\nfunction AnalyzeOntologyExtension(\n  text: string,\n  current_ontology: Ontology,\n  scope: string\n) -> OntologyExtension {\n  client CustomSonnet4\n  prompt #\"\n    You are a knowledge graph ontology expert. Your task is to analyze whether an existing ontology needs to be extended to properly extract knowledge from new text.\n    \n    {{ _.role(\"user\") }}\n    \n    CURRENT ONTOLOGY:\n    \n    Current Entity Types:\n    {% for entity_type in current_ontology.entity_types %}\n    - {{ entity_type.name }}: {{ entity_type.description }}\n    {% endfor %}\n    \n    Current Relation Types:\n    {% for relation_type in current_ontology.relation_types %}\n    - {{ relation_type.name }}: {{ relation_type.description }}\n      (Domain: {{ relation_type.domain }}, Range: {{ relation_type.range }})\n    {% endfor %}\n    \n    NEW TEXT TO ANALYZE:\n    {{ text }}\n    \n    ONTOLOGY SCOPE: {{ scope }}\n    \n    YOUR TASK:\n    \n    Analyze whether the current ontology is sufficient to extract critical information from this new text,\n    or whether it needs to be conservatively extended.\n    \n    CONSERVATIVE EXTENSION PRINCIPLES:\n    \n    1. **Default to NO Extension**\n       - Start with the assumption that the existing ontology is sufficient\n       - Only recommend extension if absolutely necessary\n       - Prefer using existing types creatively over adding new ones\n    \n    2. **Critical Information Test**\n       - Extension is ONLY justified if failing to extend would result in losing CRITICAL information\n       - Ask: \"Can the essential facts be captured with existing types?\"\n       - If yes → No extension needed\n       - If no → Consider extension\n    \n    3. **Definition of Critical Information**\n       Critical information is information that:\n       - Is central to understanding the text's main points\n       - Cannot be reasonably represented with existing entity/relation types\n       - Would create significant semantic loss if omitted or forced into existing types\n       - Is not a minor detail or edge case\n    \n    4. **Examples of When Extension IS Needed**\n       - New text discusses \"Chemical Compounds\" but ontology only has \"Substance\"\n         → If chemical-specific relationships matter, extend\n       - New text has \"Software\" entities but ontology only has \"Product\"\n         → If software-specific attributes/relations are critical, extend\n       - New text introduces entirely new domain (e.g., legal domain in a medical ontology)\n         → If domain is central to the text, extend\n    \n    5. **Examples of When Extension is NOT Needed**\n       - New text mentions \"tablets\" but ontology has \"Device\" → Use Device\n       - New text has \"acquired_by\" but ontology has \"purchased_by\" → Use existing similar relation\n       - New text mentions a rare entity type appearing once → Don't extend for one-offs\n       - Differences are purely syntactic (naming) not semantic → Don't extend\n    \n    6. **Backward Compatibility**\n       Any new types you recommend must:\n       - Be distinct from existing types (not redundant or overlapping)\n       - Follow the same naming and description conventions\n       - Work alongside existing types without conflicts\n       - Be at the appropriate granularity level for the scope\n    \n    7. **Scope Awareness**\n       Consider the current scope ({{ scope }}):\n       - For \"minimal\" scope: Be VERY conservative, only extend for major gaps\n       - For \"balanced\" scope: Extend if missing significant concepts\n       - For \"comprehensive\" scope: More willing to add nuanced types\n    \n    ANALYSIS PROCESS:\n    \n    Step 1: Identify concepts in the new text\n    Step 2: For each concept, try to map it to existing entity types\n    Step 3: For each relationship, try to map it to existing relation types\n    Step 4: Identify any critical information that CANNOT be captured\n    Step 5: If critical gaps exist, propose minimal extensions\n    Step 6: Justify why each extension is necessary\n    \n    DECISION CRITERIA:\n    \n    Set needs_extension = true ONLY if:\n    ✓ There are concepts in the text that are critically important\n    ✓ These concepts cannot be reasonably mapped to existing types\n    ✓ Omitting them would cause significant information loss\n    ✓ The extensions align with the current scope level\n    \n    Set needs_extension = false if:\n    ✓ All critical information can be captured with existing types\n    ✓ Minor details might be lost but core facts are preserved\n    ✓ Existing types can be used flexibly to cover new concepts\n    \n    OUTPUT REQUIREMENTS:\n    \n    - If needs_extension = false:\n      * Set new_entity_types and new_relation_types to empty arrays\n      * Explain in reasoning how existing types cover the text\n      * Be specific about which existing types map to which concepts\n    \n    - If needs_extension = true:\n      * Provide ONLY the new types needed (don't repeat existing ones)\n      * Describe in critical_information_at_risk what would be lost\n      * Explain in reasoning why each new type is essential\n      * Keep extensions minimal - only add what's absolutely necessary\n    \n    QUALITY CHECKS:\n    \n    Before recommending extension, verify:\n    1. Can I extract the main facts with existing types? (If yes → no extension)\n    2. Is the \"missing\" information truly critical? (If no → no extension)\n    3. Are my new types distinct from existing ones? (If not → no extension)\n    4. Am I being conservative? (If not → reconsider)\n    \n    Remember: It's better to slightly stretch existing types than to proliferate unnecessary new types.\n    The ontology should evolve slowly and deliberately, not expand with every new text.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n",
}

def get_baml_files():
    return _file_map